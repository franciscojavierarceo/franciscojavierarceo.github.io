{"pageProps":{"frontmatter":{"title":"The Difficulties of Machine Learning in Production","description":"10 lessons from a decade of deploying machine learning","date":"August 5, 2022"},"post":{"content":"\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">you know, deploying machine learning models is very, very difficult</p>&mdash; Francisco Javier Arceo (@franciscojarceo) <a href=\"https://twitter.com/franciscojarceo/status/1544110672660807680?ref_src=twsrc%5Etfw\">July 5, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n# Some History\n\nI've been working in the application of machine learning for nearly 10 years\nand it's quite exciting to have experienced firsthand how much machine learning\ninfrastructure has changed in that time.\n\nI spent the first half of my career as a \"modeler\" where I focused on building\nmachine learning and statistical models for a bunch of different use cases. Over\ntime I found that I migrated more and more to the engineering work to get the model\ndeployed because I always found this to be a bottleneck in the work I did.\n\nConcretely, I'd build a pretty good model that suggested it would be impactful\n(based on expectations of the performance) but I often found it was a rather\nextraordinary effort to get it live and interacting with users/customers.\n\nThis is quite well cited by the data science / MLOps community today but it was\nless obvious back in 2012 and it was very frustrating. The good news is that, for\nbetter or worse, I was rather relentless in getting my models over the finish line.\n\nWhich, in many ways, is what led me to the career I have stumbled into today as\nan engineering manager working at the intersection of machine learning, data,\nand engineering.\n\nAll this to say that I have spent a surprising share of the last decade working\non getting models into live product experiences. Since I have some lessons\nlearned and some potentially useful opinions, I thought I'd write them down.\n\n# Ten Lessons from a decade of Deploying ML\n\n## 0: Create strict contracts with the input source of your features\n\nThis is the single most important and most frequently omitted step in the\nmachine learning pipeline. I say this because making unit tests for schemas is\nmostly tedious work but, in my experience, this is 90% of the errors\nthat come up in production. Whether it be a missing field, new field, or a\nchange to the type of the field, it just seems to happen to *everyone* at some\npoint.\n\nWhether your model is batch/offline or real-time/online having data/schema validations\nis critical as often ML models are dependent upon data from a wide variety of\ndifferent upstream sources where the data producer has no knowledge about the\nusage of the data, so a trivial change on their end could make cause a world of\nchaos for you and the consumers of your models.\n\nSome limitations with this approach are (1) that you may not always know the\ntop-level producer of your data so making a test may not be so trivial if you\nrely on an intermediate data consumer to provide you the data or if you rely on\na vendor that changes something without your notice (this happens more often\nthan you think). Regardless of the limitations having some tests here\nwill certainly provide you more coverage and ensure higher quality.\n\n## 1: Test your feature engineering more than you want to\n\nFeature engineering is code.\n\nWhen software engineers write code for microservices we write lots of unit\ntests to capture schema changes, validate edge cases, and make the code\nmore readable.\n\nUnfortunately, these software design concepts don't always translate to\nmachine learning applications. This isn't always the case but often this happens\nbecause the skills needed to build good machine learning models are rarely\nthe same as the skills needed to write good software.\n\nSo the recommendation is simple: write lots of unit tests covering all of\nthe edge cases that come up during feature engineering.\n\nWhat are some of the edge cases?\n\n- Return type changed\n- division by zero\n- incorrect logic\n- incorrect imputation\n- incomplete if-else\n- and others\n\nIn many ways this is laborious but it makes sense because the bulk of the work\n(and code) in machine learning for tabular data is in the feature engineering\nand the pipelines supporting those features, so that is where the bulk of the\ntesting should be.\n\n## 2: Codify the range and scale of your features\n\nScale and range checking in practice can be rather challenging as sometimes\nit's very hard to know what the upper or lower bound will be but often you can\napply sensible heuristics. For example, you can test that features that are\ncalculated as ratios or rates to between [0,1] and test large but sensible\nextremum values for other features.\n\n## 3: Separate your model execution from your feature engineering\n\nPost-estimation a model is a rather trivial function call applied to a matrix\nor tensor for most algorithms. The pipeline is usually as simple as:\n\n```\n# lots of stuff above\nfeatures = build_features()\npredictions = model.fit(features=features)\n# lots of stuff  below\n```\n\nBut there are challenges that come up with the `model.fit()`. Specifically,\nsome models can be very large in size, especially ones using neural networks.\nIf you do have a large model you have to make sure you think about: right-sizing\nthe server that will hold the model in memory, where that model will be stored,\nhow the server will load the model, and what fail-over looks like.\n\nThese hardware and software considerations can get complicated on their own,\nwhich is why it's worth not mixing up the model execution with the feature\nengineering pipelines or other stuff if you don't have to.\n\n## 4: Separate matrix serialization from model execution\n\nThis is another small step in practice but is very consequential as errors come\nup and identifying the root cause can often be hairy. So I recommend that you\nseparate the matrix serialization from your model execution and feature\nengineering because it is cleaner and you'll be able to triage inevitable\nbreaks sooner.\n\nTwo specific issues that come up are (1) passing a character to a matrix and (2)\npassing a missing value (e.g., `np.nan`). (1) is not a number and in order for\nthe algorithms to do their fancy stuff they need explicit numbers and (2)\nis effectively equivalent but some matrix implementations in python will allow\nyou to hold a placeholder for a missing value and may even behave with some\nbasic operations but will ultimately fail at the `model.fit()` step.\n\nHere are two trivial examples that show this problem in action:\n```python\n# Just showing that this works as normal\nxs = np.matrix([[1,2],[3,4]])\nprint(xs * 1.3)\n#matrix([[1.3, 2.6],\n#        [3.9, 5.2]])\n\n# Example 1: Now let's try that character\nxs = np.matrix([[1,2],[3,'c']])\nprint(xs * 1.3)\n# ...it breaks! and eventually you see this confusing error:\n# numpy.core._exceptions.UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U21'), dtype('<U3')) -> None\n\n# Example 2: Now how about that np.nan?\nxs = np.matrix([[np.nan, np.nan],[np.nan, np.nan]])\nprint(xs * 1.3)\n# Wild that this works\nmatrix([[nan, nan],\n        [nan, nan]])\n```\n\nTrivial in code, chaotic when it blows up a server.\n\n## 5: Avoid mixing business logic with statistical operations\n\nMachine learning systems, even those as simple as what is effectively a\nglorified calculator can be surprisingly complicated so I generally recommend\nkeeping any business logic away from the machine learning or feature engineering\nwork. It's simply because both business logic and ml code tend to grow\ncomplicated for all sorts of valid reasons but the skills used to debug or\neven read and understand either are quite different, so the separation tends to\nhelp with that.\n\n## 6: Precompute what you can\n\nI could write an entire book about feature engineering and the complexity that\nit holds but the TL;DR version is that features tend to have all sorts of\ndifferent time representations (some feature representing real-time, others\ndata from yesterday, and others from 3 seconds ago) and this temporal component\nends up creating a lot of engineering complexity.\n\nA general recommendation: precompute and run in batch as much as you possibly\ncan. You can actually get quite far for many use cases this way without having\nto build the complexity that comes with real-time and streaming use cases.\n\nOnce you do get to real-time and streaming use cases, there's a whole other\nbunch of work to do and I won't cover that here.\n\n## 7: Load your model at service build time\n\nIn an ideal world your model is an isolated service or an isolated endpoint with\nit's own memory (maybe on kubernetes, maybe not) and you simply load it at build\ntime. It's pretty simple but this all means you could store your model artifact\nin either some [model registry](https://www.mlflow.org/docs/latest/model-registry.html)\nor in an S3 buckets.\n\nIf you're early in your model building it's absolutely fine to store the model\nin GitHub if it's not terribly huge but generally it's preferred to put it in\nsome other storage meant for larger file sizes.\n\n## 8: Right size your server\n\nGoing back again to the challenges that come up when having a large model, you\nmay not just be encountering issues there. You may also find that some of the\ndata that you have to process in real-time can get bulky (e.g., if you're\ndynamically calculating features based on user history and look back through a\nyear's worth of data for a bunch of different things).\n\nSo it's important to not only right size your server for the model that you will\nbe using but also for the features that you will be calculating in real-time.\nThis is usually fairly manageable and if you precompute a lot of features then\nyou're only doing a lookup and you'll likely have much less memory pressure but\nit is worth validating this before deploying your model.\n\n## 9: Monitor your model and features\n\nWhile this is the last item, it's an extremely important one. A general truth\nabout models is that if you're launching one it's going to be impactful to some\nprocess or customer experience (woo hoo!) but that means that people make\ntradeoffs to get things shipped and monitoring tends to be one of them.\n\nI've typically seen folks leave the monitoring of the model performance as some\nafterthought or follow up work to be completed later and usually it does and\nit's okay but it really should be top of mind. From a statistical standpoint\nthis is purely offensive as all sorts of chaotic things can go wrong\nwhen you go from some historical data to a live machine.\n\nIt could turn out that your data wasn't from a representative sample, your model\nwas trained on the wrong metric, you're adversely impacting business metrics\n(though this tends to be caught quickly), you have a bug in your code, or that\nyour model just doesn't work.\n\nYou can track most of that from monitoring, so don't omit it.\n\nMost importantly, every applied machine learning model that I've built in\nindustry has degraded (eventually). It's mostly because things change over time\nand that obviously makes sense. Without proper monitoring you will not be able\nto observe this phenomena so please don't forget this core component as it could\nbe very consequential to your business.\n\nIt's worth mentioning that model degradation not driven by engineering problems\nends up being a lot of statistical work to understand what is driving the\ndecay of models. Suffice it to say that it's a complex, high-dimensional\nproblem.\n\n\n# Some final thoughts\n\nReflecting on the list above, I can't help but call out that 9 of these 10 items\nare purely focused on the engineering around deploying a model and that the\nmonitoring is the **last** step.\n\n*As a brief aside, there is a significant amount of pre-work done here to build\nthese models during the model development lifecycle but I didn't discuss that\nhere as that's outside the scope of this blog post.*\n\nYou don't *actually* have to do anything on that list to get a model in production,\nI just recommend it. At an early stage of your service it may not even make sense\nto build a bullet-proof system but at a bigger scale these things actually do become\nincreasingly more important as preventative and defensive measures.\n\nLastly, I ranked these in order of importance and the most important ones are\nall *preventative change controls*, i.e., they can all detect breaks before you\ndeploy something to production (i.e. in unit tests). Defensive change controls\nare great too but one should remember that these will always come second place\nto preventative controls simply because you're catching a mistake after it's in\nproduction.\n\n\n*Have any feedback on this post? Let me know on the [twitter](https://twitter.com/franciscojarceo)!*\n","excerpt":""},"previousPost":{"slug":"paradoxes-and-cognitive-biases","frontmatter":{"title":"Paradoxes and Cognitive Biases","description":"A brief review of some of my favorite paradoxes and cognitive biases.","date":"May 26, 2022"},"excerpt":"","content":"\n# The Irrationality of Humans\n\nI find human behavior and decision making wildly fascinating...and mostly comical,\nespecially my own. This is because of the variety of paradoxes, cognitive biases,\nand irrationalities that are constantly at play in our micro and macro conclusions.\n\nIn fact, these irrational behaviors are largely what led me into the career I have today.\nIt all started with *economics and statistics* (my first graduate degree/love)\nand I've spent ten years working in understanding human behavior. Maybe the methods\nand engineering are fancier now than back then but, at my core, I am still\nfascinated by human behavior, data, and statistical inference—the methods that\nprovide a glimpse of understanding about a person's choices.\n\nWhat I've found is that humans are just kind of ***weird*** in their\nbehaviors and decision making. Some would say down right irrational...I certainly\nwould.\n\nMuch of that irrationality comes from all sorts of paradoxes and cognitive\nbiases that are well known in different academic disciplines (particularly\nbehavioral economics and statistics) but that aren't as well known by most\nfolks. So I thought I'd write briefly about them and why they're important.\n\nIt's worth emphasizing that even if you are aware of these things, you are still quite\nlikely to be subject to them or make the mistakes they call out—***I do all of the\ntime***. Regardless, knowing them can—to some degree—help you mitigate them\nand help you make better decisions.\n\nI would like to over-emphasize that my own decision making is nothing to brag\nabout but I decided to write this because I thought it'd be fun and I do love\nthis topic.\n\n# Paradoxes and Biases\n\nI have decided to explicitly order these in the order I felt is the most\nimportant. This, too, is ridden with bias but I believe that this is the\norder most important to me and that I've found the most useful in my\npersonal and professional life.\n\n## 00. The Dunning-Kruger Effect\n\n[The Dunning-Kruger Effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect)\nis one of the most important cognitive biases that exist, probably because of how\nimpactful it is to everyone (i.e., we all suffer from the burden of incompetent\npeople and it is quite likely we, too, are someone's burden).\n\nIn short, people with low ability at a given task tend to overestimate their\nability and those with high ability tend to underestimate it.\n\nIf you pay attention, you will see this occurrence often...especially from\nthose without much experience in a given area of expertise (though this is not\nalways true).\n\nYou may see this often from people in business...run away.\n\n## 01. The Double Standard\n\nThis is probably my favorite bias because people (like me, for example) commit\nthis often and it's such a subtle yet common thing. I believe this bias to be a\nconsequence of the Dunning-Kruger Efect mentioned above.\n\nDefinitionally, a Double Standard is \"the application of different sets of\nprinciples for situations that are, in principle, the same.\"\n\nI find this to be very important professionally as people often have unrealistic\nexpectations of others that they wouldn't have of themselves. I find this\ncomes up when folks without domain expertise are frustrated by timelines of\nfor building various things.\n\nSaid another way, *\"Why does this take so long?\"*\n\nAs a manager, I regularly ask myself \"If I were doing this, would I expect the\nsame outcome in the same time?\" and I find that this helps me better empathize\nand be more realistic about the outcomes.\n\nMore importantly, my colleagues probably find me more tolerable. :)\n\n## 02. The Curse of Knowledge\n\nI say this non-ironically: something is obvious when you know it, and not\nif you don't—so too the definition of [the Curse of Knowledge](https://en.wikipedia.org/wiki/Curse_of_knowledge).\n\nThis is something I experienced a lot in my career because people often forget\nall of the context they have when referencing something. Business is very\njargony so when I onboard people or explain things I very explicitly try to\navoid using acronyms or cryptic language. It certainly takes mental effort but\nit makes it much less frustrating for the other party.\n\nAlso, I find that assuming someone knows something or being surprised that\nsomeone *doesn't* know something can sound extremely condescending, so probably\nit may be best to avoid that.\n\n## 03. Simpson's Paradox\n\nI could write an entire post about [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\nbut, to keep it brief, Simpson's Paradox is a statistical phenomenon in which a\ncorrelation between two variables can be reversed by the addition of another.\n\nBut how??? Time for a graph!\n\n![Simpson's Paradox: Negative Correlation](simpsons_paradox_1.png)\n\nAbove variables $X$ and $Y$ are negatively correlated, quite strongly too with\na correlation coefficient of -0.74. But what if there was some other group\nvariable $Z$ which represented 5 groups, we would then be able to see:\n\n![Simpson's Paradox: Positive Correlation](simpsons_paradox_2.png)\n\nOh no! The exact opposite conclusion! It's worth knowing that in statistics and\nin life, you may never know of the existance of $Z$.\n\nSo while this is useful for regression and statistical inference, I find this\nparadox to be applicable to many more situations.\n\nSimply, I may *always* be missing a single, critical piece of information that\nmay flip my conclusion. So I tend to calibrate my opinions accordingly.\n\n## 04. The Sunk Cost Fallacy\n\nAs elegantly written by [The Decision Lab](https://thedecisionlab.com/biases/the-sunk-cost-fallacy),\n\"The Sunk Cost Fallacy describes our tendency to follow through on an endeavor\nif we have already invested time, effort, or money into it, whether or not the\ncurrent costs outweigh the benefits.\"\n\nEmotion often clutters our ability to understand the *actual* expected\nvalue/reward of a given thing we are putting effort into but sometimes it is\nin our best interest to cut our losses rather than see it through.\n\nIt rarely feels good but can often be the optimal decision.\n\n## 06. Loss Aversion\n\nLoss aversion is simply the disproportional weight a person often places on\nminimizing losses to acquire economic gains.\n\nFor example, someone may prefer to take \\$10 with 100% certainty rather than\n\\$20 with 90% certainty because the displeasure from that 10\\% possibility\noutweighs the pleasure they'd receive from the additional \\$10 (or expected\n\\$8 = (0.9 * 20) - 10).\n\nThis is extremely irrational and puts non-disciplined investors at a mathematical\ndisadvantage.\n\n## 07. The Gambler's Fallacy\n\nThe [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy)\nrefers to the incorrect belief that a given event is more\nor less likely given a previous sequence of events when the event is not a\nfunction of time.\n\nThis can be seen through coin flips. If you see 5 heads flipped in a row, you\nmay think that a tails is \"due\" but this is incorrect (assuming a fair coin) since\ncoin flips are always independent (i.e., one flip doesn't depend on the next).\n\n## 08. Anchoring\n\nPulling again from [the Decision Lab](https://thedecisionlab.com/biases/anchoring-bias),\n\"Anchoring is a cognitive bias that causes us to rely too heavily on the first\npiece of information we are given about a topic. When we are setting plans or\nmaking estimates about something, we interpret newer information from the\nreference point of our anchor, instead of seeing it objectively.\"\n\nThis is often used in marketing and pricing to delude you into thinking\nsomething is on sale. :')\n\n## 09. Sample Bias\n\nSample bias originates from statistics and is a result of a flawed collection\nof an intended random sample.\n\nThis is particularly common in business and Twitter where people think their\ncustomers or Twitter poll-responders are representative of the entire\npopulation.\n\nThey're not and this can often lead to very poor decision making or conclusions.\n\n## 10. Assignment Bias\n\n[Assignment Bias](https://www.statisticshowto.com/assignment-bias/#:~:text=Assignment%20bias%20happens%20when%20experimental,people%20who%20are%20significantly%20smarter.)\nis similar to Sample Bias in that it is a bias in the sample\nbut is rooted in a broken assignment system. For example, imagine an\nexperimental drug trial where the \"random assignment machine\" (i.e., a machine\nthat assigns things at random) only treated the young and healthy,\nwhile that is a pathological and extreme example it highlights the issue.\n\nBy the way, it turns out that a good \"random\" sample is extremely hard to\ncollect in the real world—ask the Census.\n\n## 11. Self-Selection Bias\n\n[Self-Selection Bias](https://en.wikipedia.org/wiki/Self-selection_bias)\nis another form of sample bias but it's caused by the participants choosing\nwhether or not to participate in the experiment or treatment.\n\nAgain, in the example above imagine instead that the \"random assignment\nmachine\" only treated the people who wanted to be treated and not the\nones that did.\n\nSimilar to the previous case it would ruin the experiment.\n\n## 12. Decision Fatigue\n\n[Decision Fatigue](https://en.wikipedia.org/wiki/Decision_fatigue) is a\nphenomenon whereby an individual's decision making quality deteriorates after a\nlong session of decision making.\n\nIn short, you get tired of making choices and you start to get sloppy. In the\nbusiness and investing world, this is extremely consequential because your or\nyour investor's money is on the line.\n\n## 13. Optimism Bias\n\nTo quote Wikipedia, \"[Optimism Bias](https://en.wikipedia.org/wiki/Optimism_bias)\nis a cognitive bias that causes someone to believe that they themselves are less\nlikely to experience a negative event. It is also known as unrealistic optimism\nor comparative optimism.\"\n\nIt is good to be optimistic but it is *good-er* to balance it in reality.\n\n## 14. Response Bias\n\n[Response Bias](https://en.wikipedia.org/wiki/Response_bias) is both interesting\nand counterintuitive.\n\nIt is catch-all for the frequent tendency of participants to respond\ninaccurately or falsely to survey questions. This is part of the reason surveys\noften conflict with reality.\n\nAs a statistician, I feel surverys are *kind of* useful but behaviors reveal the\ntruth. Measure behaviors.\n\nFor internet companies, you may find that user-survery metrics conflict with\ntracking metrics that you have for your customer. What people say and what they\ndo are often wildly different.\n\n## 15. The Accuracy Paradox\n\nLastly, [The Accuracy Paradox](https://en.wikipedia.org/wiki/Accuracy_paradox)\nis the paradoxical finding that Accuracy isn't necessarily a good metric for for\nmeasuring statistical or machine learning models.\n\nThis is because of an imbalance of outcomes.\n\nSuppose I was trying to predict a whether someone had a rare illness (1\nout of 10,000 people have it), if I predicted everyone didn't have it I'd still\nhave 99.99% accuracy.\n\nSo, accuracy can often be quite useless as a metric for assessing the\nquality of things in general (though not always).\n\n# Managing the Irrational\n\nYou *probably* can't completely stop yourself from irrational decision making\nbut you can *possibly* manage it.\n\nMy approach is simple: acknowledge the biases above, the idiosyncratic ones I have\nfrom my life exeriences, and reflect frequently. I find that this results in me\nchanging my mind often.\n\nThis can be frustrating but I think that early reactions or understandings\nare often not the optimal ones so I try to put effort into reflecting so that I\ncan just do better.\n\n# Some Advice\n\n(this advice may or may not be useful)\n\nBeing objective is nearly impossible when it relates to human judgement, and I'm\nnot even exactly sure what \"objective\" *really* means outside of mathematics.\n\nBut my advice on being as objective as possible is to write things down:\nbullet points, a simple pros and cons list, or whatever suits you can highlight\nflaws in your judgement and reasoning.\n\nI find this brings me mental clarity more than anything else really. I also\ntend to write things down on paper the good ol' fashion way (maybe it's an\nold habit grounded in problem sets, who knows).\n\nManaging cognitive biases and mitigating the adverse consequences they may hold\nis extremely challenging but a well worthy endeavor, as good decisions compound\nlike great investments.\n\nBut don't worry too much if you find yourself struggling with it, we are all\nhuman after all.\n\n-Francisco\n\n"},"nextPost":null},"__N_SSG":true}