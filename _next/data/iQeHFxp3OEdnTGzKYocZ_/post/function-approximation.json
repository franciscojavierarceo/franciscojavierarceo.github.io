{"pageProps":{"frontmatter":{"title":"Function Approximation using Data Science Techniques","description":"A Data Scientist's guide to Function Approximation","date":"February 21, 2021"},"post":{"content":"\n# Regression\n\nI've written about the wonders of [Linear Regression](https://franciscojavierarceo.github.io/post/ordinary-least-squares) before and one of the things I find most amazing about it is that it allows you to approximate a function.\n\n> But what does that mean?\n\nIn short, take data about two things and estimate a relationship between them.\n\nA classic example is Age and Income. \n\nSuppose you wanted to understand how a person's age *correlates* to their income. You could take a sample of data and store it in a table, spreadsheet, or even a fancy database somewhere so you could analyze it.\n\nYou could then draw a scatter plot (like below) and *visualize* the relationship between the two attributes and fit (i.e., estimate) the relationship (i.e., the slope of that line). If the relationship was strictly **linear**, we'd see a scatter plot that looks something like the graph below.\n\n![A scatter plot!](scatterplot.png)\n<p align=\"center\" style=\"padding:0\"><i>A Scatter Plot of Age and Income with a Linear Relationship</i></p>\n\nBut what if it wasn't linear?\n\nWhat if we knew Age only increased Income to a degree and that the marginal return was decreasing? Well, maybe we'd see a plot like below.\n\n![A scatter plot!](income_age_squared.jpeg)\n<p align=\"center\" style=\"padding:0\"><i>A Scatter Plot of Age and Income with a Quadratic Relationship</i></p>\n\nWhat if things were a little less intuitive and, after another point, your Income (on average) started to go back up again? We'd see the graph below.\n\n![A scatter plot!](income_age_cubic.jpeg)\n<p align=\"center\" style=\"padding:0\"><i>A Scatter Plot of Age and Income with a Cubic Relationship</i></p>\n\nLastly, what if we saw something that was just plain *weird*?\n\n![A scatter plot!](income_age_weird.jpeg)\n<p align=\"center\" style=\"padding:0\"><i>A Scatter Plot of Age and Income with a Piecewise Linear, Discontinuous Function</i></p>\n\nThis is my favorite example because it shows a [piece-wise linear function](https://en.wikipedia.org/wiki/Piecewise_linear_function) and, while strange looking, these relationships are very common phenomena in the wild.\n\nWhy?\n\nBecause often times we are modeling behaviors or decisions by other systems in the world, and those systems, decisions, and behaviors often have weird boundary points/thresholds. In the credit world, you'll often see this because a lender's credit policy systematically rejects applications with a certain set of criteria, which would lead to visualizations identical to this.\n\n## What do we do with all of this information?\n\nThis is the fun part! \n\nIf you're doing data science and modeling data with lots of features/attributes, you probably don't want to do this for hundreds or thousands of different variables. Instead, you may benefit from finding a way to estimate the univariate relationship algorithmically. \n\nBut how?\n\nWhen you have a bivariate relationship like this you don't want to have to tediously [engineer features](https://en.wikipedia.org/wiki/Feature_engineering) to estimate the underlying function, rather you'd prefer to have a machine learn (or estimate) the function using [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).\n\nBut which algorithm and how do I use it? Well, there are a few options people typically use:\n\n- [Polynomial Regression](https://en.wikipedia.org/wiki/Polynomial_regression)\n- [Multivariate Adaptive Regression Splines (MARS)](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline)\n- [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n- [Weight of Evidence](https://documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.3&docsetId=casstat&docsetTarget=casstat_binning_details03.htm&locale=en)\n\nAnd each has its own unique benefits.\n\nBut my personal favorite is MARS. If we used R's [earth package](https://cran.r-project.org/web/packages/earth/earth.pdf) we can estimate this function automatically in a few simple lines of code and get the predicted fit below.\n\n![A scatter plot!](income_age_mars.jpeg)\n<p align=\"center\" style=\"padding:0\"><i>A Scatter Plot of Age and Income using Function Approximation</i></p>\n\nAnd here's the R code to generate it.\n\n```R\nlibrary(earth)\nearth.mod <- earth(income ~ age, data = df)\nplotmo(earth.mod)\nprint(summary(earth.mod, digits = 2, style = \"pmax\"))\ndft$preds <- predict(earth.mod, df)[,1]\n```\n\nShort, sweet, and effective‚Äîmy favorite combination. \n\nThe other algorithms all have pros and cons, and I largely recommend to use each one depending on how smooth or *not*-smooth the underlying behavior of your data is (and how much you care to really account for it). \n\nAnyways, I hope you liked this post; it was a very enjoyable excuse for me to make some graphs.\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*","excerpt":""},"previousPost":{"slug":"docker-for-data-science","frontmatter":{"title":"How to use Docker to Launch a Jupyter Notebook","description":"A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook","date":"February 13, 2021"},"excerpt":"","content":"\n*TL;DR: A Data Science Tutorial on the benefits of Docker.*\n\n## Some History\nI began my foray into what's now called [Data Science](https://en.wikipedia.org/wiki/Data_science) back in 2011. I was doing my first master's in economics and statistics and I was doing econometrics research on consumer demand based on survey data using [SAS](https://www.sas.com/en_us/company-information.htmlhttps://www.sas.com/en_us/company-information/profile.html).\n\n![Look at that rise!](data-science-google-trends.png)\n<p align=\"center\" style=\"padding:0\"><i>Looks like I graduated at an interesting time.</i></p>\n\nTechnology was *much* different then, distributed computing and Open Source Software (OSS) was only starting to get the popularity and attention it has now. More practically, most businesses weren't using cloud services, they were using their own servers for storing and managing their data (i.e., real physical machines) with fixed RAM and a lot of overhead (read: chaos when the power goes out).\n\nThe most sophisticated analytical shops used SAS* to process their data since it was a very efficient way to analyze large data out of memory.\n\nBut it wasn't fault tolerant or stable. Software libraries for different mathematical frameworks have evolved so much over time and they just kept changing, so the infrastructure kept changing, too.\n\nIn short, the way data scientists did analytics was pretty brittle: most places didn't use version control or servers; code was sent via emails; and deploying models was usually done in an Oracle/MySQL table that ran a query, joins, and a sum-product. It was the Wild West.\n\nCloud computing and OSS changed the game. [R](https://cran.r-project.org), [Python](https://www.python.org), [Hadoop](https://hadoop.apache.org), [Spark](https://spark.apache.org), [CUDA](https://developer.nvidia.com/cuda-toolkit), and other frameworks completely influenced how we thought about that infrastructure.\n\nPython, in particular, has been one of the greatest contributions to data science and it has truly helped push the field further.\n\n## Python, the Beautiful\n\nPython wasn't the original data science language, R and SAS were much more popular back in the early and mid-2010s but two popular data mining libraries helped Python skyrocket in the data science community ([sklearn](https://scikit-learn.org/stable/) and [xgboost](https://en.wikipedia.org/wiki/XGBoost)). Then in 2015 people made advances in deep learning frameworks (moving away from [Theano](https://en.wikipedia.org/wiki/Theano_(software))) and creating things like [Caffe](https://caffe.berkeleyvision.org), [Keras](https://en.wikipedia.org/wiki/Keras), [Tensorflow](https://en.wikipedia.org/wiki/TensorFlow), and eventually [PyTorch](https://en.wikipedia.org/wiki/PyTorch) (my personal favorite).\n\nAll of the stuff under the hood changed dramatically and it made the infrastructure around deploying these models change dramatically, too.\n\nThe ever-evolving software made getting data science infrastructure up and running really annoying, time consuming, and eventually kind of wasteful because it would get stale quickly, but the world has evolved again.\n\nCue [Docker](https://en.wikipedia.org/wiki/Docker_(software)) and the emergence of [containerization](https://hackernoon.com/what-is-containerization-83ae53a709a6).\n\n## Docker and Jupyter Notebooks\n\nDocker is basically a way to easily configure a mini-computer in your computer. The idea being, that if you configure it with a single file declaring what stuff (i.e., software) you need in it, you can deploy that same container to some production environment. In real, customer-facing applications even a small subversion change of a single library can break your entire service.\n\nDocker can help mitigate that risk.\n\n![It works on my machine!](docker.jpg)\n<p align=\"center\" style=\"padding:0\"><i>This meme is suprisingly accurate.</i></p>\n\nWhat's interesting is that people have made attempts at doing similar things for a long time ([virtual environments](https://virtualenv.pypa.io/en/latest/), [Pyenv](https://github.com/pyenv/pyenv), [virtual box](https://www.virtualbox.org), etc.) and while most of them were helpful, they all still had really annoying issues come up constantly...but Docker is much more comprehensive.\n\nSo how is that relevant for Python and data scientists? Well, if you're doing data science work, odds are you're probably using Python or R and you might be doing that work using a [Jupyter Notebook](https://jupyter.org). \n\nThe [Jupyter Project](https://jupyter.org/about) has created a wonderful [Data Science](https://hub.docker.com/r/jupyter/datascience-notebook/) docker image that allows you to trivially get up and running.\n\nAll you have to do is [install Docker](https://docs.docker.com/engine/install/) and run the following in your terminal:\n\n```bash\ndocker run -it -p 8888:8888 -v /your/folder/:/home/jovyan/work --rm --name jupyter jupyter/datascience-notebook\n```\nYou'll see some output and at the end of it you should see something like: \n\n```\n[C 2021-02-14 14:37:06.596 ServerApp] \n    \n    To access the server, open this file in a browser:\n        file:///home/jovyan/.local/share/jupyter/runtime/jpserver-6-open.html\n    Or copy and paste one of these URLs:\n        http://7c94e4cf2dc1:8888/lab?token=this-will-be-a-magical-token\n     or http://127.0.0.1:8888/lab?token=this-will-be-a-magical-token\n```\nClick on that link in your terminal and you should be directed to a page that looks like the image below:\n\n![So many options!](docker-for-data-science.png)\n<p align=\"center\" style=\"padding:0\"><i>Who is using Julia these days?</i></p>\n\nAnd there you have it, you can start ripping through all sorts of data in minutes!\n\nA great benefit of this particular docker image is that it has most of the Python/R libraries you want already out of the box but if you want to add another, you can do that right in your notebook by using [Jupyter Magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) in a cell like so:\n```bash\n%pip install snowflake\n```\n\n![Simple install](pip_install.png)\n<p align=\"center\" style=\"padding:0\"><i>Wow, that was easy.</i></p>\n\nAnd now you can use that library. See how nice it is not to have to [dual boot](https://en.wikipedia.org/wiki/Multi-booting) your computer to install something?\n\n## Conclusion\n\nData science infrastructure is going to continue to evolve very heavily, so I imagine this post will be outdated in 2 years but currently this is an extremely fast and painless way to get up and running.\n\nI can't emphasize enough how miserable managing different Operating System (OS), Python, or other software library versions really is. Debugging these things used to take days or weeks and now it's just trivial, so I'd really recommend this approach. An added benefit is that this will also save you an extraordinary amount of time when you go to deploy your model...but more on that later. üòâ\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*\n\n---\n*As a brief side note, the history of SAS is amazing and I really recommend reading the [Wikipedia page](https://en.wikipedia.org/wiki/SAS_Institute) on it. Most of the large banks, pharmaceutical firms, several government agencies, research institutions, and other major organizations in the world still operate on SAS because it's so deeply embedded into their business. Now, that technology can no longer be decoupled from their core infrastructure, which is interesting.*"},"nextPost":{"slug":"docker-github-actions-and-cron","frontmatter":{"title":"Github Actions, Docker, and the Beloved Cron","description":"Using GitHub Actions to run a Python script inside of a docker container scheduled daily.","date":"March 14, 2021"},"excerpt":"","content":"\nI've written before about my love for [Github Actions](https://franciscojavierarceo.github.io/post/github-actions) and [Docker](https://franciscojavierarceo.github.io/post/docker-for-data-science).\n\nA little over a week ago I tweeted that I was interested in writing a blog post about using Github Actions and Docker to schedule a job to do something interesting, but I didn't have a fun use case. \n\nFortunately, my internet friends delivered.\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">text me with a pic of your dog</p>&mdash; Camilo (@camdotbio) <a href=\"https://twitter.com/camdotbio/status/1367856131972947972?ref_src=twsrc%5Etfw\">March 5, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nSo I decided to write a quick script to do this, and that's how I spent my Friday evening as my wife and I watched Avengers. üòÇ\n\nI've provided a link to the [Github Repository](https://github.com/franciscojavierarceo/twitter-cron-demo) with the full code but here's the short instructions if you're interested.\n\n## 1. Register for a developer account on [Twitter](https://developer.twitter.com/en/apply-for-access) and get your API credentials\n\nThis is a pretty easy process and you just sign up and outline what you're doing (it's also free). Note that you'll need to store 4 variables in a `.env` file like below:\n\n    TWITTER_API_KEY=\n    TWITTER_API_SECRET=\n    TWITTER_ACCESS_TOKEN=\n    TWITTER_ACCESS_TOKEN_SECRET=\n\n## 2. Write a Python Script to Tweet a Dog Photo\n\nThis was pretty straightforward thanks to the [Tweepy](https://www.tweepy.org) library in Python and this super random [dog photo API](https://dog.ceo/dog-api/) (God bless the internet).\n\nHere's what that Python code looks like:\n\n```python\nimport os\nimport json\nimport requests\nimport tweepy\n\ndef get_random_dog(filename: str='temp') -> None:\n    r = requests.get('https://dog.ceo/api/breeds/image/random')\n    rd = json.loads(r.content)\n    r2 = requests.get(rd['message'])\n\n    with open(filename, 'wb') as image:\n        for chunk in r2:\n            image.write(chunk)\n\ndef main(message: str, filename: str='temp') -> None:\n    auth = tweepy.OAuthHandler(\n        os.environ.get('TWITTER_API_KEY'), \n        os.environ.get('TWITTER_API_SECRET')\n    )\n    auth.set_access_token(\n        os.environ.get('TWITTER_ACCESS_TOKEN'), \n        os.environ.get(\"TWITTER_ACCESS_TOKEN_SECRET\")\n    )\n    api = tweepy.API(auth)\n    get_random_dog(filename) \n\n    try:\n        api.verify_credentials()\n        print(\"Twitter Authentication Succeeded\")\n    \n        try:\n            api.update_with_media(filename, status=message)\n            print('Tweet successfully sent!')\n\n        except Exception as e:\n            print('Error sending tweet \\n %s' % e)\n    except:\n        print(\"Twitter AUthentication Failed\")\n\n\nif __name__ == '__main__':\n    main(\"Hey, @camdotbio! üëã \\n\\nHere's your daily dog photo!\")\n```\n\n## 3. Create a Docker Container to run the Python Script \n\nThis is a contentious area where some may argue docker is execessive for this but I use different computers with different operating systems so this works for me and I like it. \n\nIn short, I created a [Dockerfile](https://github.com/franciscojavierarceo/twitter-cron-demo/blob/main/Dockerfile) and a [docker-compose](https://github.com/franciscojavierarceo/twitter-cron-demo/blob/main/docker-compose.yml) file where I run the script. The benefit of this is that I don't have to worry about this script not working on my Linux machine or not working on my Mac, it works on both!\n\n## 4. Use a Github Action to Schedule a Cron Job\n\nThis was also straightforward and I've copied the code below:\n\n```yml\nname: Build and Deploy üöÄ\n\non:\n    schedule:\n        - cron: '0 15 * * *'\n\njobs:\n    build:\n        runs-on: ubuntu-latest\n\n        steps:\n            - uses: actions/checkout@v2\n\n            - name: Make envfile\n              uses: SpicyPizza/create-envfile@v1\n              with:\n                  envkey_TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}\n                  envkey_TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}\n                  envkey_TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}\n                  envkey_TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}\n                  file_name: .env\n\n            - name: Build the docker container\n              run: docker build .\n\n            - name: Run the script üöÄ\n              run: docker-compose up\n```\nNote that you have to create [Action Secrets](https://docs.github.com/en/actions/reference/encrypted-secrets) in your Github Repository (available in the Settings tab) and add the credentials from Step 1.\n\n## Conclusion \n\nAnd that's it! üê∂ \n\nPushing the code to GitHub handles the rest, isn't that wonderful?\n\nAnyways, thanks to my internet friends for the fun idea. I didn't end up sending it to Camilo's phone number because I would have to pay Twilio $0.0075.\n\n---\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*"}},"__N_SSG":true}