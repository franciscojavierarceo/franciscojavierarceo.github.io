{"pageProps":{"frontmatter":{"title":"How to build a credit risk model","description":"A data scientists guide to building a basic credit risk model","date":"January 27, 2021"},"post":{"content":"\n\n*TL;DR: A Data Science Tutorial on building a Credit Risk model*\n\nI previously [wrote](/post/data-science-and-fintech) about some of the work data scientists do in the fintech space, which briefly discussed [credit risk models](https://en.wikipedia.org/wiki/Credit_risk) but I wanted to write a more technical review and talk about what I think are the most important points.\n\nI spent most of my career at banks as a data scientist and built credit risk models in the consumer and commercial space in the US, Australia, and South Africa. I learned a lot from those experiences, so I thought I'd share a simple example of how one of the core pieces of algorithmic/quantitative underwriting is done.\n\nIn this post, I'll try to answer the following questions:\n\n- What is a credit risk model?\n- What **data** does a credit risk model use?\n- How do you estimate a credit risk model?\n- How do you know if your model is performing well?\n- What are some common mistakes to avoid?\n- What are useful facts to know about credit risk models?\n\n## What is a Credit Risk Model?\n\nIn the consumer/retail space, a credit risk model tries to predict the probability that a consumer won't repay money that they've borrowed.\nA simple example of this is an [unsecured personal loan](https://www.investopedia.com/terms/u/unsecuredloan.asp). \nLet's suppose you submitted an application to borrow $5,000 from a lender. That lender would want to know the likelihood of [default](https://www.investopedia.com/terms/d/default2.asp) before deciding on (1) whether to give you the loan and (2) the price they want to charge you to for borrowing the money. So that probability is probably quite important...but how do they come up with it?\n\n## What Data does a Credit Risk model use?\n\nLenders typically use data from the major [Credit Bureaus](https://www.investopedia.com/personal-finance/top-three-credit-bureaus/) that is [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act) compliant, which basically means that the legal and reputational risk of using this data is very low.\n\nTypically, you'd purchase a dataset from one of the bureaus (or use data inside one of their analytical sandboxes) and clean the dataset into something that looks like the following:\n\n<table>\n  <tr>\n    <th>Default</th>\n    <th>Inquiries in Last 6 Months</th>\n    <th>Credit Utilization</th>\n    <th>Average Age of Credit</th>\n    <th>...</th>\n  </tr>\n  <tr>\n    <td>Yes</td>\n    <td>2</td>\n    <td>0.8</td>\n    <td>12</td>\n    <td>...</td>\n  </tr>\n  <tr>\n    <td>No</td>\n    <td>8</td>\n    <td>0.0</td>\n    <td>2</td>\n    <td>...</td>\n  </tr>\n  <tr>\n    <td>...</td>\n    <td>...</td>\n    <td>...</td>\n    <td>...</td>\n    <td>...</td>\n  </tr>\n</table>\n\nAnd so on.\n\nIn summary, it's just a bunch of data about your borrowing history.\nIt's a little recursive/cyclical because in order to grow your credit you need to have credit but let's ignore that detail.\n\nOne of the most important steps in the model development process is *precisely* defining **default** because this will eventually reflect the performance of your portfolio, so defining it rigorously, accurately, and confidently is extremely consequential.\n\nSo how do you define it?\n\nTime. \n\nIf you're planning to launch a term loan, you usually set boundaries on the duration of the loan.\nLet's say you want to launch a 12 month loan to an underserved market, then you'd want to get historical data of other lenders to build your model on.\nIt sounds a little surprising that you can do this but that's basically how the bureaus make their money.\n\nAn important thing to keep in mind is that you need to make sure you pull the data at 2 different time periods: (1) when the original application was made so you can use data that is relevant for underwriting (and so you don't have forward-looking data resulting in [data leakage](https://www.kaggle.com/dansbecker/data-leakage)) and (2) 12 months later (or whatever time period is appopriate for you) to check if the consumer defaulted on their loan.\n\nThere's a lot more to it and you can expand on things in much more elegant ways to handle different phenomena but for the sake of simplicity, this is essentially how it's done.\n\nSo, after painfully cleaning up all that data, what do you do with it?\n\n## Building a Probability of Default Model\n\nNow that you have that glorious dataset you can start to run different [Logistic Regressions](https://en.wikipedia.org/wiki/Logistic_regression) or use other classification based machine learning algorithms to find hidden patterns and relationships (i.e., [non-linear functions](https://blog.minitab.com/blog/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis#:~:text=If%20the%20equation%20doesn't,a%20linear%20equation%2C%20it's%20nonlinear.&text=Thetas%20represent%20the%20parameters%20and,one%20parameter%20per%20predictor%20variable.) and [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics))).\nPersonally, this is the most intellectually engaging part of the work. The other work involved in credit risk modeling is usually very stressful and filled with less exciting graphs but here you get to pause, look at data, and, for a moment, try to make a crude approximation of the world. I find this part *fascinating*.\n\nIf we stick with our simple model above, we could use our good old friend Python to run a simple regression.\n\n```python\nimport numpy as np\nimport statsmodels.api as sm\n\nn = 10000\nnp.random.seed(0)\nx_1 = np.random.poisson(lam=5, size=n)\nx_2 = np.random.poisson(lam=2, size=n)\nx_3 = np.random.poisson(lam=12, size=n)\ne = np.random.normal(size=n, loc=0, scale=1.)\n\nb_1, b_2, b_3 = -0.005, -0.03, -0.15\nylogpred =  x_1 * b_1 + x_2 * b_2 + x_3 * b_3 + e\nyprob = 1./ (1.+ np.exp(-ylogpred))\nyclass = np.where(yprob >= 0.5, 1, 0)\nxs = np.hstack([\n    x_1.reshape(n, 1), \n    x_2.reshape(n, 1), \n    x_3.reshape(n, 1)\n])\nxs = sm.add_constant(xs)\nmodel = sm.Logit(y, x)\nres = model.fit()\nprint(res.summary())\n         Current function value: 0.163863\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                10000\nModel:                          Logit   Df Residuals:                     9996\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 29 Jan 2021   Pseudo R-squ.:                  0.1056\nTime:                        00:08:17   Log-Likelihood:                -1638.6\nconverged:                       True   LL-Null:                       -1832.2\nCovariance Type:            nonrobust   LLR p-value:                 1.419e-83\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4535      0.209      2.166      0.030       0.043       0.864\nx1            -0.0439      0.023     -1.949      0.051      -0.088       0.000\nx2            -0.0390      0.035     -1.109      0.267      -0.108       0.030\nx3            -0.3065      0.017    -18.045      0.000      -0.340      -0.273\n==============================================================================\n```\n\nWow, look at all of that beautiful, useless statistical output! It's not really useless but 99% of the people involved will not find it useful. \nSo we probably need an alternative way to show and inform these results to non-technical stakeholders (but your data scientist can look at this as much as they'd like).\n\n## Evaluating your Default Model\n- AUC and GINI\n\n## Common Mistakes and Pitfalls\n- Simpson's Paradox\n\n## Other Important pieces\n- Panel Data and the Discrete-Time Hazard Model\n- Generating Adverse Actions codes has a variety of different approaches\n- Does a more complicated model impact my target population?\n\n## Conclusion\n","excerpt":""},"previousPost":{"slug":"data-science-and-fintech","frontmatter":{"title":"Data Science and Fintech","description":"How Data Science Scaled the Fintech Revolution","date":"January 22, 2021"},"excerpt":"","content":"\nI've spent the last 10 years working in data science, mostly in finance and technology (fintech) and it's been really exciting to see how data science, engineering, and the internet has reshaped all aspects of finance.\n\n[Others have written before](https://fintechtoday.substack.com/p/part-1-what-is-fintech-30-anyway) about how fintech has evolved over the last decade and one area that I think is interesting is how data science and analytics has helped fuel that growth.\n\n>So, how ***exactly*** has data science helped scale fintech?\n\nI think data science has scaled fintech in five key areas.\n\n## 1. Operations<a name=\"Operations\"></a>\n\nThe market tends to have a preference for technology companies because of the operational efficiencies that come from technology's scale. Simply put, data scientists help identify data and processes that can be automated to help achieve better operational efficiency.\n\nA concrete fintech example of this is fraud operations. If you're a bank with a credit card product, manually reviewing even 10% of your credit card transactions would be an impossible, and costly, task (it's been [cited](https://www.marketwatch.com/story/why-bitcoin-wont-displace-visa-or-mastercard-soon-2017-12-15) that Visa and Mastercard process 5,000 transactions per second, which would mean that there are 5000 * 60 * 60 * 24=432,000,000 transactions per day to go through).\n\nSo, in this circumstance data scientists build technology and predictive models to reduce the amount of manual review.\n\n## 2. Marketing<a name=\"Marketing\"></a>\n\nIn fintech, Customer Acquisition Cost (CAC) is everything. Others have written about [CAC and fintech](https://medium.com/unifimoney/the-no-cac-bank-5e0e577d5473) in greater depth, but suffice it to say it is a challenging and competitive problem.\n\nData scientists focused on marketing try to reduce CAC through a wide variety of strategies.\nSome of them are by tightly monitoring product metrics to see which features yield the best ROI for growth, while other approaches take a broader lense by taking a comprehensive view of your marketing investments and, again, optimize the expected return (e.g., through [Marketing Mix Models](https://blog.hurree.co/blog/marketing-mix-modeling)).\n\nOther marketing focused approaches use [propensity models](https://medium.com/the-official-integrate-ai-blog/heres-what-you-need-to-know-about-propensity-modeling-521ab660cb43) to try to maximize customer engagement or acquisition. More concretely, this can involve building a propensity model to convert a customer from an email subscriber to a fully-converted user (e.g., for a lending product or a mobile application), while other propensities may focus on simply getting a customer to re-engage with your product.\n\n## 3. Risk Management<a name=\"Risk-Management\"></a>\n\nThis is where I've spent most of my career and I think it's a really hard problem that most fintechs struggle with in the lending space.\nGenerally speaking, data scientists will build risk models (e.g., for credit risk or fraud risk) to predict the probability of default or some likelihood of delinquency ([more on the difference between them](https://www.investopedia.com/ask/answers/062315/what-are-differences-between-delinquency-and-default.asp)).\n\nBuilding good predictive models is hard. Building good *risk* models is **extremely** hard.\n\nThis is less because of a technology or data problem and more because of regulatory checks and balances in place. Making sure that you adhere to [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act), [ECOA](https://uscode.house.gov/view.xhtml?req=granuleid%3AUSC-prelim-title15-chapter41-subchapter4&edition=prelim), and other regulatory oversight is hard on its own, adding statistical analysis into the mix makes it more challenging.\n\nImplementation (i.e., getting an algorithm into production that impacts your customers) of these models is a whole other area of data science and one of the areas I personally find quite fun (maybe I'll write more about this topic later).\n\n## 4. Technology<a name=\"Technology\"></a>\n\nData scientists often work with engineering/technology teams in order to improve the technology stack. This may involve changing an architecture to reduce the latency of certain [microservices](https://microservices.io/) or enhancing the curent stack for a unique problem (cue machine learning and [Airflow DAGs](https://airflow.apache.org)).\nWhile some of this is behind the scenes, it can be some of the most impactful work done by a data scientist in the fintech space because of the broader impact to the core business.\n\n## 5. Product<a name=\"Product\"></a>\n\nData scientists working with product teams are often tasked with the measurement of different experiences within the product and finding out ways to enhance it. That can vary from creating dashboards to monitor the right metrics, to building a recommendation system to curate something specific for a user. \n\nData scientists can fuel product growth, which is why [Facebook, Google, Amazon, Microsoft](https://www.datasciencedegreeprograms.net/lists/five-of-the-largest-companies-that-employ-data-scientists/) and other tech companies hire so many data scientists.\n\nIt's worth noting that I've ignored many of the technology and organizational complexities involved when hiring or building data science teams but only because I wanted to keep this post high-level to introduce some of the applications of data science to fintech. In the future, I'll probably write more technical posts of each one of these to give more concrete examples with code and diagrams (which is what I find fun ðŸ˜Š). \n\nTo summarize, data scientists are useful (particularly in fintech) when there are hard problems and data available to solve them.\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*"},"nextPost":null},"__N_SSG":true}