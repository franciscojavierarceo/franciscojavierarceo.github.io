<!DOCTYPE html><html lang="en-US"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if(!e)return localStorage.setItem('theme','system'),d.add('system');if("system"===e){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else d.add(e)}catch(e){}}()</script><title>Data Science and Fintech</title><meta name="description" content="How Data Science Scaled the Fintech Revolution"/><meta property="og:type" content="website"/><meta name="og:title" property="og:title" content="Data Science and Fintech"/><meta name="og:description" property="og:description" content="How Data Science Scaled the Fintech Revolution"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Data Science and Fintech"/><meta name="twitter:description" content="How Data Science Scaled the Fintech Revolution"/><meta name="twitter:creator" content="franciscojarceo"/><link rel="icon" type="image/png" href="/favicon.ico"/><link rel="apple-touch-icon" href="/favicon.ico"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/b7d29a021b3498b7a4a5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b7d29a021b3498b7a4a5.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-39418da170c6bb422e22.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.a6f6c255f30e2cc8ab2c.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-ed60e04ec8eae61be051.js" as="script"/><link rel="preload" href="/_next/static/chunks/5e7de4f36e438e169c5d145e7df90137ae956f1e.c55db21543619f2e3353.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/post/%5Bslug%5D-691413d01639bc43c6a6.js" as="script"/></head><body><div id="__next"><div class="w-full min-h-screen dark:bg-gray-700 dark:text-white"><div class="max-w-screen-md px-4 py-12 mx-auto antialiased font-body"><header class="flex items-center justify-between  mb-2"><meta name="description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="keywords" content="francisco javier arceo, data science, finance, fintech, engineering, django, python"/><meta property="og:type" content="website"/><meta property="og:title" content="Francisco&#x27;s Random Thoughts"/><meta property="og:locale" content="en_US"/><meta property="og:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta property="og:image" content="https://og-image.now.sh/Francisco&#x27;s%20Random%20Thoughts.png?theme=light&amp;md=0&amp;fontSize=75px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="twitter:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="franciscojarceo"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-71125809-1/"></script><script src="/ga.js"></script><div class="max-w-md"><a><a class="text-2xl font-black text-black no-underline font-display dark:text-white" href="/">Francisco Javier Arceo</a></a></div><script></script></header><main style="padding-bottom:10px"><article><header class="mb-8"><h1 class="mb-2 text-6xl font-black leading-none font-display">Data Science and Fintech</h1><p class="text-sm">January 22, 2021</p></header><div class="mb-4 prose lg:prose-lg dark:prose-dark"><p>I&#x27;ve spent the last 10 years working in data science, mostly in finance and technology (fintech) and it&#x27;s been really exciting to see how data science, engineering, and the internet has reshaped all aspects of finance.</p><p><a href="https://fintechtoday.substack.com/p/part-1-what-is-fintech-30-anyway">Others have written before</a> about how fintech has evolved over the last decade and one area that I think is interesting is how data science and analytics has helped fuel that growth.</p><blockquote><p>So, how <em><strong>exactly</strong></em> has data science helped scale fintech?</p></blockquote><p>I think data science has scaled fintech in five key areas.</p><h2>1. Operations<a name="Operations"></a></h2><p>The market tends to have a preference for technology companies because of the operational efficiencies that come from technology&#x27;s scale. Simply put, data scientists help identify data and processes that can be automated to help achieve better operational efficiency.</p><p>A concrete fintech example of this is fraud operations. If you&#x27;re a bank with a credit card product, manually reviewing even 10% of your credit card transactions would be an impossible, and costly, task (it&#x27;s been <a href="https://www.marketwatch.com/story/why-bitcoin-wont-displace-visa-or-mastercard-soon-2017-12-15">cited</a> that Visa and Mastercard process 5,000 transactions per second, which would mean that there are 5000 * 60 * 60 * 24=432,000,000 transactions per day to go through).</p><p>So, in this circumstance data scientists build technology and predictive models to reduce the amount of manual review.</p><h2>2. Marketing<a name="Marketing"></a></h2><p>In fintech, Customer Acquisition Cost (CAC) is everything. Others have written about <a href="https://medium.com/unifimoney/the-no-cac-bank-5e0e577d5473">CAC and fintech</a> in greater depth, but suffice it to say it is a challenging and competitive problem.</p><p>Data scientists focused on marketing try to reduce CAC through a wide variety of strategies.
Some of them are by tightly monitoring product metrics to see which features yield the best ROI for growth, while other approaches take a broader lense by taking a comprehensive view of your marketing investments and, again, optimize the expected return (e.g., through <a href="https://blog.hurree.co/blog/marketing-mix-modeling">Marketing Mix Models</a>).</p><p>Other marketing focused approaches use <a href="https://medium.com/the-official-integrate-ai-blog/heres-what-you-need-to-know-about-propensity-modeling-521ab660cb43">propensity models</a> to try to maximize customer engagement or acquisition. More concretely, this can involve building a propensity model to convert a customer from an email subscriber to a fully-converted user (e.g., for a lending product or a mobile application), while other propensities may focus on simply getting a customer to re-engage with your product.</p><h2>3. Risk Management<a name="Risk-Management"></a></h2><p>This is where I&#x27;ve spent most of my career and I think it&#x27;s a really hard problem that most fintechs struggle with in the lending space.
Generally speaking, data scientists will build risk models (e.g., for credit risk or fraud risk) to predict the probability of default or some likelihood of delinquency (<a href="https://www.investopedia.com/ask/answers/062315/what-are-differences-between-delinquency-and-default.asp">more on the difference between them</a>).</p><p>Building good predictive models is hard. Building good <em>risk</em> models is <strong>extremely</strong> hard.</p><p>This is less because of a technology or data problem and more because of regulatory checks and balances in place. Making sure that you adhere to <a href="https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act">FCRA</a>, <a href="https://uscode.house.gov/view.xhtml?req=granuleid%3AUSC-prelim-title15-chapter41-subchapter4&amp;edition=prelim">ECOA</a>, and other regulatory oversight is hard on its own, adding statistical analysis into the mix makes it more challenging.</p><p>Implementation (i.e., getting an algorithm into production that impacts your customers) of these models is a whole other area of data science and one of the areas I personally find quite fun (maybe I&#x27;ll write more about this topic later).</p><h2>4. Technology<a name="Technology"></a></h2><p>Data scientists often work with engineering/technology teams in order to improve the technology stack. This may involve changing an architecture to reduce the latency of certain <a href="https://microservices.io/">microservices</a> or enhancing the curent stack for a unique problem (cue machine learning and <a href="https://airflow.apache.org">Airflow DAGs</a>).
While some of this is behind the scenes, it can be some of the most impactful work done by a data scientist in the fintech space because of the broader impact to the core business.</p><h2>5. Product<a name="Product"></a></h2><p>Data scientists working with product teams are often tasked with the measurement of different experiences within the product and finding out ways to enhance it. That can vary from creating dashboards to monitor the right metrics, to building a recommendation system to curate something specific for a user.</p><p>Data scientists can fuel product growth, which is why <a href="https://www.datasciencedegreeprograms.net/lists/five-of-the-largest-companies-that-employ-data-scientists/">Facebook, Google, Amazon, Microsoft</a> and other tech companies hire so many data scientists.</p><p>It&#x27;s worth noting that I&#x27;ve ignored many of the technology and organizational complexities involved when hiring or building data science teams but only because I wanted to keep this post high-level to introduce some of the applications of data science to fintech. In the future, I&#x27;ll probably write more technical posts of each one of these to give more concrete examples with code and diagrams (which is what I find fun üòä).</p><p>To summarize, data scientists are useful (particularly in fintech) when there are hard problems and data available to solve them.</p><p><em>Have some feedback? Feel free to <a href="https://twitter.com/franciscojarceo">let me know</a>!</em></p></div></article></main><footer class="text-lg font-light"><div style="padding-top:10px;padding-bottom:10px"><a class="text-lg font-bold" href="/">‚Üê Back home</a></div><hr/><div><p>Like this blog? Check out the code on my<!-- --> <a href="https://github.com/franciscojavierarceo/franciscojavierarceo.github.io">GitHub</a>.</p><p>Built with<!-- --> <a href="https://nextjs.org/">Next.js</a> and ‚òï</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Data Science and Fintech","description":"How Data Science Scaled the Fintech Revolution","date":"January 22, 2021"},"post":{"content":"\nI've spent the last 10 years working in data science, mostly in finance and technology (fintech) and it's been really exciting to see how data science, engineering, and the internet has reshaped all aspects of finance.\n\n[Others have written before](https://fintechtoday.substack.com/p/part-1-what-is-fintech-30-anyway) about how fintech has evolved over the last decade and one area that I think is interesting is how data science and analytics has helped fuel that growth.\n\n\u003eSo, how ***exactly*** has data science helped scale fintech?\n\nI think data science has scaled fintech in five key areas.\n\n## 1. Operations\u003ca name=\"Operations\"\u003e\u003c/a\u003e\n\nThe market tends to have a preference for technology companies because of the operational efficiencies that come from technology's scale. Simply put, data scientists help identify data and processes that can be automated to help achieve better operational efficiency.\n\nA concrete fintech example of this is fraud operations. If you're a bank with a credit card product, manually reviewing even 10% of your credit card transactions would be an impossible, and costly, task (it's been [cited](https://www.marketwatch.com/story/why-bitcoin-wont-displace-visa-or-mastercard-soon-2017-12-15) that Visa and Mastercard process 5,000 transactions per second, which would mean that there are 5000 * 60 * 60 * 24=432,000,000 transactions per day to go through).\n\nSo, in this circumstance data scientists build technology and predictive models to reduce the amount of manual review.\n\n## 2. Marketing\u003ca name=\"Marketing\"\u003e\u003c/a\u003e\n\nIn fintech, Customer Acquisition Cost (CAC) is everything. Others have written about [CAC and fintech](https://medium.com/unifimoney/the-no-cac-bank-5e0e577d5473) in greater depth, but suffice it to say it is a challenging and competitive problem.\n\nData scientists focused on marketing try to reduce CAC through a wide variety of strategies.\nSome of them are by tightly monitoring product metrics to see which features yield the best ROI for growth, while other approaches take a broader lense by taking a comprehensive view of your marketing investments and, again, optimize the expected return (e.g., through [Marketing Mix Models](https://blog.hurree.co/blog/marketing-mix-modeling)).\n\nOther marketing focused approaches use [propensity models](https://medium.com/the-official-integrate-ai-blog/heres-what-you-need-to-know-about-propensity-modeling-521ab660cb43) to try to maximize customer engagement or acquisition. More concretely, this can involve building a propensity model to convert a customer from an email subscriber to a fully-converted user (e.g., for a lending product or a mobile application), while other propensities may focus on simply getting a customer to re-engage with your product.\n\n## 3. Risk Management\u003ca name=\"Risk-Management\"\u003e\u003c/a\u003e\n\nThis is where I've spent most of my career and I think it's a really hard problem that most fintechs struggle with in the lending space.\nGenerally speaking, data scientists will build risk models (e.g., for credit risk or fraud risk) to predict the probability of default or some likelihood of delinquency ([more on the difference between them](https://www.investopedia.com/ask/answers/062315/what-are-differences-between-delinquency-and-default.asp)).\n\nBuilding good predictive models is hard. Building good *risk* models is **extremely** hard.\n\nThis is less because of a technology or data problem and more because of regulatory checks and balances in place. Making sure that you adhere to [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act), [ECOA](https://uscode.house.gov/view.xhtml?req=granuleid%3AUSC-prelim-title15-chapter41-subchapter4\u0026edition=prelim), and other regulatory oversight is hard on its own, adding statistical analysis into the mix makes it more challenging.\n\nImplementation (i.e., getting an algorithm into production that impacts your customers) of these models is a whole other area of data science and one of the areas I personally find quite fun (maybe I'll write more about this topic later).\n\n## 4. Technology\u003ca name=\"Technology\"\u003e\u003c/a\u003e\n\nData scientists often work with engineering/technology teams in order to improve the technology stack. This may involve changing an architecture to reduce the latency of certain [microservices](https://microservices.io/) or enhancing the curent stack for a unique problem (cue machine learning and [Airflow DAGs](https://airflow.apache.org)).\nWhile some of this is behind the scenes, it can be some of the most impactful work done by a data scientist in the fintech space because of the broader impact to the core business.\n\n## 5. Product\u003ca name=\"Product\"\u003e\u003c/a\u003e\n\nData scientists working with product teams are often tasked with the measurement of different experiences within the product and finding out ways to enhance it. That can vary from creating dashboards to monitor the right metrics, to building a recommendation system to curate something specific for a user. \n\nData scientists can fuel product growth, which is why [Facebook, Google, Amazon, Microsoft](https://www.datasciencedegreeprograms.net/lists/five-of-the-largest-companies-that-employ-data-scientists/) and other tech companies hire so many data scientists.\n\nIt's worth noting that I've ignored many of the technology and organizational complexities involved when hiring or building data science teams but only because I wanted to keep this post high-level to introduce some of the applications of data science to fintech. In the future, I'll probably write more technical posts of each one of these to give more concrete examples with code and diagrams (which is what I find fun üòä). \n\nTo summarize, data scientists are useful (particularly in fintech) when there are hard problems and data available to solve them.\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*","excerpt":""},"previousPost":{"slug":"ordinary-least-squares","frontmatter":{"title":"Ordinary Least Squares","description":"A brief note about the most important equation in all of statistics.","date":"January 14, 2021"},"excerpt":"","content":"\nOne of my favorite authors and historical statisticians [Dr. Stephen Stigler](https://stat.uchicago.edu/people/profile/stephen-m.-stigler/) published a wonderful historical review in 1981 titled [*Gauss and the Invention of Least Squares*](https://projecteuclid.org/download/pdf_1/euclid.aos/1176345451). He argued that the prolific [Carl Freidrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) discovered [Ordinary Least Squares](https://en.wikipedia.org/wiki/Least_squares) (OLS) in 1809 and fundamentally shaped the future of science, business, and society as we know it.\n\nSo, what is OLS and why is it so important?\n\nOLS is often referred to by many things across several different discipilines, some of them are:\n\n- Linear Regression\n- Multivariate Regression\n- The Normal Equations\n- Maximum Likelihood\n- Method of Moments\n- Singular Value Decomposition of $\\bf{X}\\bf{w}-\\bf{y}=U(\\Sigma'\\bf{w}-U'-\\bf{y})$\n\nBut all of them ultimately reflect the same mathematical expression (in scalar notation):\n\n$$y_i = \\beta_0 + \\sum_{j=1}^{k} \\beta_i x_i + \\epsilon_i$$\n\nWhich yields the famous estimator (i.e., equation) for $\\hat{\\beta_j}$ as\n\n$$\\hat{\\beta_j} = \\sum_{i=1}^{n} (x_i - y_i)^2 / \\sum_{i=1}^n (x_i - \\bar{x})^2$$\n\nOr in matrix notation:\n\n$$\\bf \\hat{\\beta} = \\bf (X'X)^{-1} X'Y$$.\n\nI find this simple equation to be so extraordinary.\n\nWhy? Because of what can be learned from it: the equation basically says \"Look at data about $\\bf{x}$ and estimate a linear relationship to $\\bf{y}$\". \n\nAs a concrete example, imagine you wanted to know the relationship between age and income (a simplification of the well-studied [Mincer Equation](https://en.wikipedia.org/wiki/Mincer_earnings_function)), how would you figure this out? A simple linear regression could estimate that relationship and the $\\hat{\\beta}$ would represent the partial-correlation (sometimes called the marginal effect or coefficient estimate) and it exactly represents the slope of the line below.\n\n![A scatter plot!](scatterplot.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eA Scatter Plot of Age and Income\u003c/i\u003e\u003c/p\u003e\n\nIsn't that just amazing??\n\nThis single expression is used to estimate models for movie recommendations, businesses, pharmaceuticals, and even decisions about public health. I am constantly amazed at how one little equation could accomplish so much.\n\nTo think Gauss had discovered OLS as a method of calculating the orbits of celestial bodies and that today, over 200 years later, humans would use it to for so much of what we do is astounding.\n\nOver the years statisticians, economists, computer scientists, engineers, and psychometricians have advanced OLS in such profound and unique ways. Some of them have been used to reflect data generated from more non-standard distributions (e.g., a [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution)), or to frame the problem to use prior information in a structured way (e.g., through [Bayesian Inference](https://en.wikipedia.org/wiki/Bayesian_inference)), while others have enhanced these equations to learn high-dimensional non-linear relationships (e.g., via [Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)). Again, all of these are extended from the extraordinary work of Gauss.\n\nThere's so much that can be written about all of the advancements that have been made in all of these fields and a short blog post simply won't do it justice, but I thought I'd at least share some thoughts about it.\n\nSomewhere along the way today I came across something related to important equations and it led me to write this, so I hope you enjoyed it. \n\nI'm such a fan of the history of statistics and mathematics that this piece, while not as structured as I'd like, was very enjoyable to write.\n\nHappy computing!\n\n-Francisco"},"nextPost":{"slug":"how-to-build-a-credit-risk-model","frontmatter":{"title":"How to Build a Credit Risk Model","description":"A Data Scientists Guide to Building a Basic Credit Risk Model","date":"January 31, 2021"},"excerpt":"","content":"\n*TL;DR: A Data Science Tutorial on building a Credit Risk model.*\n\nI previously [wrote](/post/data-science-and-fintech) about some of the work data scientists do in the fintech space, which briefly discussed [credit risk models](https://en.wikipedia.org/wiki/Credit_risk) but I wanted to write a more technical review and talk about what I think are the most important points.\n\nI spent most of my career at banks as a data scientist and built credit risk, fraud, and marketing models in the consumer and commercial space in the US, Australia, and South Africa. I learned a lot from those experiences, so I thought I'd share a simple example of how one of the core pieces of algorithmic/quantitative underwriting is done.\n\nIn this post, I'll try to answer the following questions:\n\n- What is a credit risk model?\n- What data does a credit risk model use?\n- How do you estimate a credit risk model?\n- How do you know if your model is performing well?\n- What are some common mistakes to avoid?\n- What are useful facts to know about credit risk models?\n\nIt's worth caveating up front that this is a very narrow take focused only on the analytical aspect of the work and there is an extraordinary amount of legal, compliance, and business work that I am intentionally omitting.\n\nWith that said, let's dive right in.\n\n## What is a Credit Risk Model?\n\nIn the consumer/retail space, a credit risk model tries to predict the probability that a consumer won't repay money that they've borrowed.\nA simple example of this is an [unsecured personal loan](https://www.investopedia.com/terms/u/unsecuredloan.asp). \n\nLet's suppose you submitted an application to borrow $5,000 from a lender. That lender would want to know the likelihood of [default](https://www.investopedia.com/terms/d/default2.asp) before deciding on (1) whether to give you the loan and (2) the price they want to charge you for borrowing the money. So that probability is probably quite important...but how do they come up with it?\n\n## What Data does a Credit Risk Model Use?\n\nLenders typically use data from the major [Credit Bureaus](https://www.investopedia.com/personal-finance/top-three-credit-bureaus/) that is [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act) compliant, which basically means that the legal and reputational risk of using this data is very low.\n\nTypically, you'd purchase a dataset from one of the bureaus (or use data inside one of their analytical sandboxes) and clean the dataset into something that looks like the following:\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eDefault\u003c/th\u003e\n    \u003cth\u003eInquiries in Last 6 Months\u003c/th\u003e\n    \u003cth\u003eCredit Utilization\u003c/th\u003e\n    \u003cth\u003eAverage Age of Credit\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eYes\u003c/td\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e0.8\u003c/td\u003e\n    \u003ctd\u003e12\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eNo\u003c/td\u003e\n    \u003ctd\u003e8\u003c/td\u003e\n    \u003ctd\u003e0.0\u003c/td\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nAnd so on.\n\nIn summary, it's just a bunch of data about your borrowing history.\nIt's a little recursive/cyclical because in order to grow your credit you need to have credit but let's ignore that detail.\n\nOne of the most important steps in the model development process is *precisely* defining **default** because this will eventually reflect the performance of your portfolio defining it thoughtfully, accurately, and confidently is extremely consequential.\n\nSo how do you define it?\n\nTime. \n\nIf you're planning to launch a term loan, you usually set boundaries on the duration of the loan. For revolving lines of credit (like a credit card), you simply apply a reasonable boundary on time that makes good business sense.\n\nLet's say you want to launch a 12 month loan to an underserved market, then you'd want to get historical data of other lenders to build your model on.\nIt sounds a little surprising that you can do this but that's basically how the bureaus make their money.\n\nAn important thing to keep in mind is that you need to make sure you pull the data at 2 different time periods: (1) when the original application was made so you can use data that is relevant for underwriting (and so you don't have forward-looking data resulting in [data leakage](https://www.kaggle.com/dansbecker/data-leakage)) and (2) 12 months later (or whatever time period is appropriate for you) to check if the consumer defaulted on their loan.\n\nThere's a lot more to it and you can expand on things in much more elegant ways to handle different phenomena but for the sake of simplicity, this is essentially how it's done.\n\nSo, after painfully cleaning up all that data, what do you do with it?\n\n## Building a Probability of Default Model\n\nNow that you have that glorious dataset you can start to run different [Logistic Regressions](https://en.wikipedia.org/wiki/Logistic_regression) or use other classification based [machine learning](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html) algorithms to find hidden patterns and relationships (i.e., [non-linear functions](https://blog.minitab.com/blog/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis#:~:text=If%20the%20equation%20doesn't,a%20linear%20equation%2C%20it's%20nonlinear.\u0026text=Thetas%20represent%20the%20parameters%20and,one%20parameter%20per%20predictor%20variable.) and [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics))).\n\nPersonally, this is the most intellectually engaging part of the work. The other work involved in credit risk modeling is usually very stressful and filled with less exciting graphs but here you get to pause, look at data, and, for a moment, try to make a crude approximation of the world. I find this part *fascinating*.\n\nIf we stick with our simple dataset above for our model, we could use our good old friend Python to run that Logistic Regression.\n\n```python\nimport numpy as np\nimport statsmodels.api as sm\n\n# Generating the data \nn = 10000\nnp.random.seed(0)\nx_1 = np.random.poisson(lam=5, size=n)\nx_2 = np.random.poisson(lam=2, size=n)\nx_3 = np.random.poisson(lam=12, size=n)\ne = np.random.normal(size=n, loc=0, scale=1.)\n\n# Setting the coefficient values to give us a ~5% default rate\nb_1, b_2, b_3 = -0.005, -0.03, -0.15\nylogpred =  x_1 * b_1 + x_2 * b_2 + x_3 * b_3 + e\nyprob = 1./ (1.+ np.exp(-ylogpred))\nyclass = np.where(yprob \u003e= 0.5, 1, 0)\nxs = np.hstack([\n    x_1.reshape(n, 1), \n    x_2.reshape(n, 1), \n    x_3.reshape(n, 1)\n])\n# Adding an intercept to the matrix\nxs = sm.add_constant(xs)\nmodel = sm.Logit(yclass, xs)\n# All that work just to run .fit(), how terribly uninteresting\nres = model.fit()\nprint(res.summary())\n\n         Current function value: 0.163863\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                10000\nModel:                          Logit   Df Residuals:                     9996\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 29 Jan 2021   Pseudo R-squ.:                  0.1056\nTime:                        00:08:17   Log-Likelihood:                -1638.6\nconverged:                       True   LL-Null:                       -1832.2\nCovariance Type:            nonrobust   LLR p-value:                 1.419e-83\n==============================================================================\n                 coef    std err          z      P\u003e|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4535      0.209      2.166      0.030       0.043       0.864\nx1            -0.0439      0.023     -1.949      0.051      -0.088       0.000\nx2            -0.0390      0.035     -1.109      0.267      -0.108       0.030\nx3            -0.3065      0.017    -18.045      0.000      -0.340      -0.273\n==============================================================================\n```\n\nWow, look at all of that beautiful, useless statistical output! \n\nIt's not *really* useless but 99% of the people involved will not find it useful. \nSo we probably need an alternative way to show and inform these results to non-technical stakeholders (but you as a data scientist can look at this as much as you'd like).\n\n![The Glorious Lift Chart!](liftchart.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eThe Beloved Lift Chart\u003c/i\u003e\u003c/p\u003e\n\nCue the [Lift Chart](https://www.casact.org/education/rpm/2016/presentations/PM-LM-4.pdf). This chart may look fancy but it's actually pretty simple, it's just [deciling](https://www.investopedia.com/terms/d/decile.asp) your data (i.e., sorting and bucketing into 10 equal groups) according to the *predicted* default rate from your model. It's worth noting that this is a quantized and reversed version of the [ROC chart](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and they represent the same information.\n\nThere are 3 important pieces of information from this graph: \n\n1. The AUC tells you the performance\n2. The closer the two lines are together the more accurate the probability estimate\n3. The steeper the slope at the higher deciles, the better the rank order separation\n\nIf you had perfect information, you'd get a graph that looks like this:\n\n![The Perfect Lift Chart!](liftchart_optimal.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eIf your model looks like this, you've done something terribly wrong\u003c/i\u003e\u003c/p\u003e\n\nAnd here's the code to generate those graphs.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\ndef liftchart(df: pd.DataFrame, actual: str, predicted: str, buckets: int=10) -\u003e None:\n    # Bucketing the predictions (Deciling is the default)\n    df['predbucket'] = pd.qcut(x=df[predicted], q=buckets)\n    sdf = df[[actual, predicted, 'predbucket']].groupby(\n        by=['predbucket']).agg({\n        actual: [np.mean, sum, len], \n        predicted: np.mean\n        }\n    )\n    aucperf = roc_auc_score(df[actual], df[predicted])\n    sdf.columns = sdf.columns.map(''.join) # I hate pandas multi-indexing\n    sdf = sdf.rename({\n        actual + 'mean': 'Actual Default Rate', \n        predicted + 'mean': 'Predicted Default Rate'\n    }, axis=1)\n    sdf[['Actual Default Rate', 'Predicted Default Rate']].plot(\n        kind='line', style='.-', grid=True, figsize=(12, 8), \n        color=['red', 'blue']\n    )\n    plt.ylabel('Default Rate')\n    plt.xlabel('Decile Value of Predicted Default')\n    plt.title('Actual vs Predicted Default Rate sorted by Predicted Decile \\nAUC = %.3f' % aucperf)\n    plt.xticks(\n        np.arange(sdf.shape[0]), \n        sdf['Predicted Default Rate'].round(3)\n    )\n    plt.show()\n\n# Using the data and model from before!\npdf = pd.DataFrame(xs, columns=['intercept', 'x1', 'x2', 'x3'])\npdf['preds'] = res.predict(xs)\npdf['actual'] = yclass\n\n# Finally, what we all came here to see\nliftchart(pdf, 'actual', 'preds')\n\n# This is what it looks like when we have perfect information\npdf['truth'] = pdf['actual'] + np.random.uniform(low=0, high=0.001, size=pdf.shape[0])\nliftchart(pdf, 'actual', 'truth', 10)\n```\n\u003ccenter\u003e\u003ci\u003eJudge me not by the elegance of my code but by the fact that it runs.\u003c/i\u003e\u003c/center\u003e\n\n## Evaluating your Default Model\n\nSo this visualization is helpful, but how do you quantify the performance into a single number? Well, how about [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)?\n\nIt turns out that Accuracy isn't really a great metric when you have a low default rate (or more generally when you have severe [class imbalance](https://en.wikipedia.org/wiki/Accuracy_paradox)). As an example suppose you have a 5% default rate, that means 95% of your data did *not default*, so if your model predicted that no one defaulted, you'd have 95% accuracy.\n\nAccurate, obvious, and entirely useless. Without proper adjustment, this behavior is actually very likely to occur in your model, so we tend to ignore the accuracy metric and instead we focus on the rank order seperation/predictive power of the model.\n\nTo measure that predictive power, there are 4 metrics industry professionals typically look at to summarize their model: [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), the [Kolomogorov-Smirnov (KS) Test](https://en.wikipedia.org/wiki/Kolmogorov‚ÄìSmirnov_test), and [Gini/AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n\nOne point of humor that has been a surprisingly common topic of discussion in my career is the equivalence of Gini and AUC. Bankers like Gini, for whatever inertia related reason, but it's equivalent to AUC via:\n\n$$Gini = 2 * AUC - 1$$ \n\nand obviously\n\n$$AUC = \\frac{Gini+1}{2}$$.\n\nGini is bound between [-1, 1] and AUC between [0, 1] but technically if your AUC is less than 0.5 (and \u003c 0 for Gini) that means you're doing worse than random ([and you could do better by literally doing the opposite of what your model says](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)) so most people will say AUC is between [0.5, 1] and that Gini is between [0, 1].\n\nA good model is usually around 70% AUC / 40% Gini. The higher the better.\n\nIt's always useful to look at your lift chart as it can tell you a lot about how predictive the model is at different deciles. In some cases, your model may not have enough variation in the attributes/features/predictors to differentiate your data meaningfully. The metric won't show you that, only a glance at the lift chart will, so it's always a good to review it.\n\nOne important point here is that any single metric is very crude and a data set can be pathologically constructed to break it, so while these metrics and charts are very helpful, there are cases where things can still misbehave even though they seem normal.\n\nNow that you have your exciting new default model, you can use it in your underwriting strategy to differentiate amongst your competitors on pricing, approve/decline, and targeting customers, right?\n\nNot quite.\n\n## Common Mistakes and Pitfalls\n\nBefore you get too excited about your model, you want to verify that it's behaving logically, so I thought I'd list some items that you will want to check against to make sure nothing disastrous will happen. \n\n- Double check for [Data Leakage](https://www.kaggle.com/dansbecker/data-leakage)\n- Verify that you don't have any [data errors](https://www.datasciencecentral.com/profiles/blogs/common-errors-in-machine-learning-due-to-poor-statistics-knowledg)\n- Make sure you've done your best not to fall for [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\n- Validate your model with an [Out of Time](https://statmodeling.stat.columbia.edu/2016/11/22/30560/) Hold Out Sample\n- Confirm your model actually has a [representative sample](https://www.investopedia.com/terms/r/representative-sample.asp#:~:text=A%20representative%20sample%20is%20a,three%20males%20and%20three%20females.) of your future portfolio\n- Make sure you're not time traveling \n    - This is a form of Data Leakage (Leaky Predictors) and it's basically making sure you don't use data that's in the future of what you're representing (except the default flags that you're trying to predict/model)\n- Understand the correlation your *predicted default* may have to your other products in your portfolio\n    - This is not important from a statistical perspective but it is very important for your business\n- Always be skeptical if your model performs too well\n    - This is very likely data leakage and can be very embarrassing if you over-excite management\n\nI could have written a book ([recommendation here](https://www.amazon.com/Credit-Risk-Modelling-Theoretical-Foundations-Diagnostic-ebook/dp/B07FZGG63V)) with a much longer set of information both from a statistical and business lense but, for the sake of brevity, I wrote the most pressing ones and I invite you to search for more information about other important details.\n\n## Some other Interesting Extensions\n\n\n### Model Extensions\nThe model I showed above is quite literally the most basic example and the models that most lenders and vendors use are much, much more sophisticated. Many of them have enhanced their models to handle [panel data](https://en.wikipedia.org/wiki/Panel_data) (i.e., cohorts of people over time) and can construct models that are extremely robust.\n\nThe best class of models in this elite category (in my personal opinion) are [Discrete Time Survival Models](https://bookdown.org/content/4253/fitting-basic-discrete-time-hazard-models.html). Actually, this isn't *my* opinon, this wisdom was shared with me from two of the former heads of statistical modeling at Capital One, so don't trust my judgement, trust theirs.\n\n### Quantization and the Weight of Evidence\nAnother interesting implementation detail is that statisticians/data scientists in the credit scoring space often transform the inputs/attributes/features of their model via [quantization](https://en.wikipedia.org/wiki/Quantization). More specifically through a transformation called the [Weight of Evidence](https://multithreaded.stitchfix.com/blog/2015/08/13/weight-of-evidence/). It's neat, troglodytic, and surprisingly effective.\n\n\n### Adverse Action Reasons\nCredit risk modeling requires the underwriter to grant adverse action codes in the case of rejecting a customer who applies for a credit product (this is a driven by the FCRA). Doing this algorithmitically is very doable and different organizations do it slightly differently but the short version is you take the absolute largest partial predictions (i.e., $argmax_{x}f(x_j) = |x_j * \\beta_j|$,  $\\forall j=1,..., k$).\n\n### Model Complexity and Usefulness\nLastly, when building these statistical models the most important thing to think about is the trade-off between complexity and performance on your target population. Typically in the lending space you will not approve all of your customers, so obsessing over model fit on the proportion of your population that won't be approved is not always useful. It's a small point, but extremely consequential and will hopefully save you some time.\n\n## Conclusion\n\nThis tutorial was not nearly as code heavy as I would have liked (only showing a very silly regression) but I felt like the content was still rather dense ([the full code is available here](https://github.com/franciscojavierarceo/Python/blob/master/demos/credit-model-demo.py)).\n\nPeople spend their careers studying these statistical models and the important consequences of them, so I really want to emphasize how much I have trivialized the work here only for the sake of simplicity. I have not talked about how you can improve the model when it's underperforming or how one can try to measure biases that may occur, but these are important pieces for society.\n\nI will conclude by saying that these models are heavily governed by many layers of regulation. As a statistician and a minority, I think this is a good thing.\nThat's not to say that harm isn't caused by these models because I think there is but I do think machines are more easily governed, evaluated, and modified...unlike human decision making which can be subjective and prone to unknown biases that are difficult to quantify ([here's some additional reading material](https://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf)).\n\nI hope you found this post useful and interesting.\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*"}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"data-science-and-fintech"},"buildId":"khW1lbdeLrJ3NzbCm6Y_3","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-fa276ba060a4a8ac7eef.js"></script><script src="/_next/static/chunks/main-39418da170c6bb422e22.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.a6f6c255f30e2cc8ab2c.js" async=""></script><script src="/_next/static/chunks/pages/_app-ed60e04ec8eae61be051.js" async=""></script><script src="/_next/static/chunks/5e7de4f36e438e169c5d145e7df90137ae956f1e.c55db21543619f2e3353.js" async=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-691413d01639bc43c6a6.js" async=""></script><script src="/_next/static/khW1lbdeLrJ3NzbCm6Y_3/_buildManifest.js" async=""></script><script src="/_next/static/khW1lbdeLrJ3NzbCm6Y_3/_ssgManifest.js" async=""></script></body></html>