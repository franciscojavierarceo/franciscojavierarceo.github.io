<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Customer Segmentation using Data Science</title><meta name="description" content="A Data Scientists Guide to Segmenting your Customers using clustering algorithms and decision trees."/><meta property="og:type" content="website"/><meta name="og:title" property="og:title" content="Customer Segmentation using Data Science"/><meta name="og:description" property="og:description" content="A Data Scientists Guide to Segmenting your Customers using clustering algorithms and decision trees."/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Customer Segmentation using Data Science"/><meta name="twitter:description" content="A Data Scientists Guide to Segmenting your Customers using clustering algorithms and decision trees."/><meta name="twitter:creator" content="franciscojarceo"/><link rel="icon" type="image/png" href="/favicon.ico"/><link rel="apple-touch-icon" href="/favicon.ico"/><link rel="preload" href="/assets/inertia.png" as="image" fetchPriority="high"/><link rel="preload" href="/assets/kmeans.png" as="image" fetchPriority="high"/><link rel="preload" href="/assets/decisiontree.png" as="image" fetchPriority="high"/><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/2ddc893e6f67eaa4.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/2ddc893e6f67eaa4.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-2555a4296ab7a1b2.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-fae63b21a27d6472.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-de2c74f8722a8d2e.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-0eb7e304a6637746.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/401-6e60ee3ab13a84ce.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/165-4ec2600001888911.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-14754cb6b0504a35.js" defer="" crossorigin=""></script><script src="/_next/static/kJxfpRQQyGIN4m6m9HTPj/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/kJxfpRQQyGIN4m6m9HTPj/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="w-full min-h-screen dark:bg-dark-bg dark:text-white"><div class="max-w-screen-md px-4 py-12 mx-auto antialiased font-body"><header class="flex items-center justify-between  mb-2"><meta name="description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="keywords" content="francisco javier arceo, data science, finance, fintech, engineering, django, python"/><meta property="og:type" content="website"/><meta property="og:title" content="Francisco&#x27;s Random Thoughts"/><meta property="og:locale" content="en_US"/><meta property="og:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta property="og:image" content="https://og-image.now.sh/Francisco&#x27;s%20Random%20Thoughts.png?theme=light&amp;md=0&amp;fontSize=75px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="twitter:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="franciscojarceo"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-71125809-1/"></script><script src="/ga.js"></script><div class="max-w-md"><a class="text-2xl font-black text-black no-underline font-display dark:text-white" href="/">Francisco Javier Arceo</a></div><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"Francisco Javier Arceo","alternateName":"Francisco&apos;s Random Thoughts","alumniOf":{"@type":"CollegeOrUniversity","name":["Columbia University in the City of New York","Clemson University","Illinois State University"]},"knowsAbout":["Django","Data Science","Statistics","Machine Learning","Economics","Econometrics","Computer Science","Natural Language Processing"]}</script></header><main style="padding-bottom:10px"><article><header class="mb-8"><h1 class="mb-2 text-6xl font-black leading-none font-display">Customer Segmentation using Data Science</h1><p class="text-sm">February 6, 2021</p></header><div class="mb-4 prose lg:prose-lg dark:prose-dark"><p><em>TL;DR: A Data Science Tutorial on using K-Means and Decision Trees together.</em></p>
<p>Customer segmentation (sometimes called <a href="https://en.wikipedia.org/wiki/Market_segmentation">Market Segmentation</a>) is ubiqutous in the private sector. We think about bucketing people into <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> mutually exclusive and collectively exhausting (MECE) groups. The premise being that instead of having 1 strategy for delivering a product or experience, providing <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> experiences or strategies will yield much better engagement or acquisition from our customers.</p>
<p>Generally speaking, this makes sense; it&#x27;s intuitive. Provide people a more curated experience and they will enjoy it more...and the more personalized the better.</p>
<p>Netflix, Spotify, YouTube, Twiter, Instagram, and all of the big tech companies have mastered personalization by using robust, computationally expensive, and sophisticated machine learning pipelines. But the world has been doing this for a long time, just a much less sophisticated version.</p>
<p>So I thought I&#x27;d give a technical demo of what customer segmentation looks like in a basic way using a trick I&#x27;ve used for years.</p>
<p>Here are the things I&#x27;d like to cover during this demo:</p>
<ol>
<li>What options do I have to segment my customers?</li>
<li>How do I actually do the segmentation?</li>
<li>What can I do with my new customer segments?</li>
<li>How do I know that my segments are effective?</li>
<li>How do I know when my segments have changed?</li>
</ol>
<h2>Approaches to Customer Segmentation</h2>
<p>The phrase &quot;Customer Segments&quot; tends to mean different things across different industries, organizations, and even across business functions (e.g., marketing, risk, product, etc.).</p>
<p>As an example, for a consumer products retailer, they may refer to customer segments using both demographic information or their purchase behavior, where a lender may refer to their segments based on credit score bands. While very meaningfully different from a business perspective, the same algorithms can be used for both problems.</p>
<p>Analytically speaking, I&#x27;ve seen Customer Segments defined really in two main ways: (1) Business Segments and (2) Algorithmic Segments. Usually executives refer to their segments in the first category and data scientists focus on the second. The first is really important organizationally because 99% of the people working with your customers don&#x27;t care about how you bucketed them and customers are the most important thing. Always.</p>
<p>...but how do you <em>actually</em> (i.e., in code and data) get to those segments?</p>
<h3>1. Logical Business Segments</h3>
<p>These segments tend to be defined by heuristics and things that make common sense. They are often built on things that are aligned with the goal of the business.</p>
<p>Here are some examples:</p>
<ul>
<li>The age of the customer (in years)</li>
<li>The income of the customer (in dollars or thousands of dollars)</li>
<li>The amount of money a customer spent in the last year</li>
<li>The likelihood a customer will spend money at a given store (purchase propensity / propensity to buy)</li>
<li>The customer&#x27;s geographic region (e.g., zipcode, state)</li>
</ul>
<p>In data, some of that customer information would look something like this:</p>
<!-- -->&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;User ID&lt;/th&gt;
    &lt;th&gt;Age&lt;/th&gt;
    &lt;th&gt;Customer Income&lt;/th&gt;
    &lt;th&gt;Purchase Propensity&lt;/th&gt;
    &lt;th&gt;...&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;1&lt;/td&gt;
    &lt;td&gt;25&lt;/td&gt;
    &lt;td&gt;$45,000&lt;/td&gt;
    &lt;td&gt;0.9&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;2&lt;/td&gt;
    &lt;td&gt;30&lt;/td&gt;
    &lt;td&gt;$80,000&lt;/td&gt;
    &lt;td&gt;0.4&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;n&lt;/td&gt;
    &lt;td&gt;56&lt;/td&gt;
    &lt;td&gt;$57,000&lt;/td&gt;
    &lt;td&gt;0.1&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
And so on.<!-- -->
<p>We could apply some logic/rules/code to create segment like:</p>
<ul>
<li>Age Buckets<!-- -->
<ol>
<li>&lt; 25</li>
<li>25-35</li>
<li>35-55</li>
<li>55+</li>
</ol>
</li>
<li>Income Buckets<!-- -->
<ol>
<li>&lt; $25K</li>
<li>$25K-50K</li>
<li>$50K-100K</li>
<li>$100-150K</li>
<li>$150K+</li>
</ol>
</li>
<li>Propensity Buckets<!-- -->
<ol>
<li>Low: [0, 0.25]</li>
<li>Medium: [0.25, 0.75]</li>
<li>High: [0.75, 1.0]</li>
</ol>
</li>
</ul>
<p>And map that logic into our data, which would yield</p>
<!-- -->&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;User ID&lt;/th&gt;
    &lt;th&gt;Age Bucket&lt;/th&gt;
    &lt;th&gt;Income Bucket&lt;/th&gt;
    &lt;th&gt;Propensity Bucket&lt;/th&gt;
    &lt;th&gt;...&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;1&lt;/td&gt;
    &lt;td&gt;25-35&lt;/td&gt;
    &lt;td&gt;$25K-50K&lt;/td&gt;
    &lt;td&gt;High&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;2&lt;/td&gt;
    &lt;td&gt;25-35&lt;/td&gt;
    &lt;td&gt;$50K-100K&lt;/td&gt;
    &lt;td&gt;Medium&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;n&lt;/td&gt;
    &lt;td&gt;56&lt;/td&gt;
    &lt;td&gt;$50K-100K&lt;/td&gt;
    &lt;td&gt;Low&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
And so on.<!-- -->
<p>Pretty simple, right? The code for this categorization is simple too (assuming you&#x27;re using Pandas and Python; though it&#x27;s also simple in SQL).</p>
<pre><pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre></pre>
<p>This is a really helpful and simple way to understand our customers and it&#x27;s the way that most businesses do analytics, but we can do more. üòä</p>
<h3>2. Algorithmic Segments</h3>
<p>Segments defined using simple business logic are great because they are so easy to interpret, but that&#x27;s not free.
By favoring simplicity we have to limit ourselves to (potentially) suboptimal segments.
This is typically on purpose and entirely fine but, again, we can do better.</p>
<p>So how do we do better?</p>
<p>Cue statistics, data mining, analytics, machine learning, or whatever it&#x27;s called this week. More specifically, we can use the classic <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-Means Clustering</a> algorithm to <em>learn</em> an optimal set of segments given some set of data.</p>
<p>To skip over many important details (<a href="https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3">more here</a>), K-Means is an algorithm that optimally buckets your data into <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> groups (according to a specific mathematical function called the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">euclidean distance</a>). It&#x27;s a classic approach and tends to work quiet well in practice (there are a ton of other neat <a href="https://en.wikipedia.org/wiki/Cluster_analysis#Algorithms">clustering algorithms</a>) but one non-technical challenge is (1) choosing <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> and (2) explaining what a single cluster actually means to literally anyone else.</p>
<p>Solving (1) is relatively straight-forward. You can run K-means for some number of <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> from [0, <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre>] (<pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> and choose what appears to be a <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> that sufficiently minimizes the within-cluster sum-of-squares (i.e., <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre>). Here notice that the the majority of the variation of the clusters can be capture by <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre>.</p>
<p><img alt="The Inertia Function!" fetchPriority="high" width="800" height="400" decoding="async" data-nimg="1" class="w-full" style="color:transparent" src="/assets/inertia.png"/></p>
<!-- -->&lt;p align=&quot;center&quot; style=&quot;padding:0&quot;&gt;&lt;i&gt;Inertia as a function of k&lt;/i&gt;&lt;/p&gt;<!-- -->
<p>Now to (2), which is the harder challenge. If I were to plot my data and look at the clusters, I&#x27;d have something that looks like:</p>
<p><img alt="K-Means!" fetchPriority="high" width="800" height="400" decoding="async" data-nimg="1" class="w-full" style="color:transparent" src="/assets/kmeans.png"/></p>
<!-- -->&lt;p align=&quot;center&quot; style=&quot;padding:0&quot;&gt;&lt;i&gt;Look at all 3 of those beautiful dimensions!&lt;/i&gt;&lt;/p&gt;<!-- -->
<p>How cool, right? This little algorithm learned pretty clear groups that you can see rather obviously in the data. Impressive! And also useless to your boss and colleagues.</p>
<p>More seriously, while you can see these clusters, you can&#x27;t actually extract a clear description from it, which makes interpreting it really, really hard when you go past 3 dimensions.</p>
<p>So what can you do to make this slightly more meaningful?</p>
<p>Enter <a href="https://en.wikipedia.org/wiki/Decision_tree">decision trees</a>. Another elegant, classic, and amazing algorithm. Decision Trees basically split up your data using simple <pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre> statements. So, a trick that you can use is to take the predicted clusters and run a Decision Tree (Classification) to predict the segment and use the learneed Tree&#x27;s logic as your new business logic.</p>
<p>I find this little trick pretty fun and effective since I can more easily describe how a machine learned a segment and I can also inspect it. Let&#x27;s suppose I ran my tree on this learned K-means, what would the output look like?</p>
<p><img alt="Decision Tree Ouput!" fetchPriority="high" width="800" height="400" decoding="async" data-nimg="1" class="w-full" style="color:transparent" src="/assets/decisiontree.png"/></p>
<!-- -->&lt;p align=&quot;center&quot; style=&quot;padding:0&quot;&gt;&lt;i&gt;Is this really more interpretable?&lt;/i&gt;&lt;/p&gt;<!-- -->
<p>There you have it, now you have a segmentation that is closer to optimal and somewhat easier to interpret. It&#x27;s still not as good as the business definition but you could actually read through this and eventually come up with a heuristic driven approach as well, which is why I like it and why I&#x27;ve used it in the past.</p>
<p>And here&#x27;s the code to run the K-means and the Decision tree.</p>
<pre><pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre></pre>
<h2>What can you do with your new segments?</h2>
<p>Now that we have our customer segments we can do all sorts of different things.
We can create <a href="https://www.optimizely.com/optimization-glossary/ab-testing/">A/B tests</a> for website experiences or we can test the impact of <a href="https://medium.com/@judaikawa/price-elasticity-statistical-modeling-in-the-retail-industry-a-quick-overview-fdab5350222">changing our prices</a> to certain customers.
In general, we can just try a bunch of new stuff.</p>
<h2>How do I know if my segments are accurate?</h2>
<p>The metric we used in the example above (i.e., within cluster sum-of-squares / inertia) was a reasonably straightforward way to measure the accuracy of your segments from an analytical perspective, but if you wanted to take a closer look, I&#x27;d recommend reviewing individual users in each segment. It sounds a little silly and can, in some cases, lead to the wrong conclusions but I firmly believe that in data science, you just have to really <strong>look</strong> at your data. You learn a lot from it.</p>
<h2>How do I know when my segments need to change?</h2>
<p>Lastly, segments can change; your customers are always evolving so it&#x27;s good to re-evaluate your clusters time and again. The emergence of new segments should feel very obvious, since it may be driven by product or acquisition changes. As a concrete example, if you noticed that important businesss metrics split by your segments are starting to behave a little differently, then you can investigate whether it&#x27;s driven by a change in the segments; sometimes it is, sometimes it&#x27;s not.</p>
<h2>Conclusion</h2>
<p>This tutorial ended up being a little longer than I anticipated but oh well, I hope you enjoyed it.</p>
<p>I&#x27;ve stored the code to reproduce this example in a <a href="https://github.com/franciscojavierarceo/Python/blob/master/demos/Customer%20Segmentation%20Example.ipynb">Jupyter Notebook</a> available on my GitHub (note to render the interactive 3D visualization you have to run the notebook). To get it up and running you only need to download the notebook, <a href="https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python">download the data</a>, install <a href="https://www.docker.com/get-started">Docker</a>, and simply run:</p>
<pre><pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="white-space:pre;color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span></span></code></pre></pre>
<p>And you should be good to go. Happy segmenting!</p>
<p><em>Have some feedback? Feel free to <a href="https://twitter.com/franciscojarceo">let me know</a>!</em></p></div></article></main><footer class="text-lg font-light"><div style="padding-top:10px;padding-bottom:10px"><a class="text-lg font-bold" href="/">‚Üê Back home</a></div><hr/><div><p>Like this blog? Check out the code on my<!-- --> <a href="https://github.com/franciscojavierarceo/franciscojavierarceo.github.io" class="text-link-blue">GitHub</a>.</p><p>Built with<!-- --> <a href="https://nextjs.org/" class="text-link-blue">Next.js</a> and ‚òï</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"frontmatter":{"title":"Customer Segmentation using Data Science","description":"A Data Scientists Guide to Segmenting your Customers using clustering algorithms and decision trees.","date":"February 6, 2021"},"post":{"content":"\n*TL;DR: A Data Science Tutorial on using K-Means and Decision Trees together.*\n\nCustomer segmentation (sometimes called [Market Segmentation](https://en.wikipedia.org/wiki/Market_segmentation)) is ubiqutous in the private sector. We think about bucketing people into $k$ mutually exclusive and collectively exhausting (MECE) groups. The premise being that instead of having 1 strategy for delivering a product or experience, providing $k$ experiences or strategies will yield much better engagement or acquisition from our customers.\n\nGenerally speaking, this makes sense; it's intuitive. Provide people a more curated experience and they will enjoy it more...and the more personalized the better. \n\nNetflix, Spotify, YouTube, Twiter, Instagram, and all of the big tech companies have mastered personalization by using robust, computationally expensive, and sophisticated machine learning pipelines. But the world has been doing this for a long time, just a much less sophisticated version.\n\nSo I thought I'd give a technical demo of what customer segmentation looks like in a basic way using a trick I've used for years.\n\nHere are the things I'd like to cover during this demo:\n\n1. What options do I have to segment my customers?\n2. How do I actually do the segmentation?\n3. What can I do with my new customer segments?\n4. How do I know that my segments are effective?\n5. How do I know when my segments have changed?\n\n## Approaches to Customer Segmentation\n\nThe phrase \"Customer Segments\" tends to mean different things across different industries, organizations, and even across business functions (e.g., marketing, risk, product, etc.). \n\nAs an example, for a consumer products retailer, they may refer to customer segments using both demographic information or their purchase behavior, where a lender may refer to their segments based on credit score bands. While very meaningfully different from a business perspective, the same algorithms can be used for both problems.\n\nAnalytically speaking, I've seen Customer Segments defined really in two main ways: (1) Business Segments and (2) Algorithmic Segments. Usually executives refer to their segments in the first category and data scientists focus on the second. The first is really important organizationally because 99% of the people working with your customers don't care about how you bucketed them and customers are the most important thing. Always.\n\n...but how do you *actually* (i.e., in code and data) get to those segments?\n\n### 1. Logical Business Segments\nThese segments tend to be defined by heuristics and things that make common sense. They are often built on things that are aligned with the goal of the business.\n\nHere are some examples:\n\n- The age of the customer (in years)\n- The income of the customer (in dollars or thousands of dollars)\n- The amount of money a customer spent in the last year\n- The likelihood a customer will spend money at a given store (purchase propensity / propensity to buy)\n- The customer's geographic region (e.g., zipcode, state)\n\nIn data, some of that customer information would look something like this:\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eUser ID\u003c/th\u003e\n    \u003cth\u003eAge\u003c/th\u003e\n    \u003cth\u003eCustomer Income\u003c/th\u003e\n    \u003cth\u003ePurchase Propensity\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e1\u003c/td\u003e\n    \u003ctd\u003e25\u003c/td\u003e\n    \u003ctd\u003e$45,000\u003c/td\u003e\n    \u003ctd\u003e0.9\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e30\u003c/td\u003e\n    \u003ctd\u003e$80,000\u003c/td\u003e\n    \u003ctd\u003e0.4\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en\u003c/td\u003e\n    \u003ctd\u003e56\u003c/td\u003e\n    \u003ctd\u003e$57,000\u003c/td\u003e\n    \u003ctd\u003e0.1\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\nAnd so on.\n\nWe could apply some logic/rules/code to create segment like:\n\n- Age Buckets\n    1. \u003c 25\n    2. 25-35\n    3. 35-55\n    4. 55+\n- Income Buckets\n    1. \u003c $25K\n    2. $25K-50K\n    3. $50K-100K\n    4. $100-150K\n    5. $150K+\n- Propensity Buckets\n    1. Low: [0, 0.25]\n    2. Medium: [0.25, 0.75]\n    3. High: [0.75, 1.0]\n\nAnd map that logic into our data, which would yield\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eUser ID\u003c/th\u003e\n    \u003cth\u003eAge Bucket\u003c/th\u003e\n    \u003cth\u003eIncome Bucket\u003c/th\u003e\n    \u003cth\u003ePropensity Bucket\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e1\u003c/td\u003e\n    \u003ctd\u003e25-35\u003c/td\u003e\n    \u003ctd\u003e$25K-50K\u003c/td\u003e\n    \u003ctd\u003eHigh\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e25-35\u003c/td\u003e\n    \u003ctd\u003e$50K-100K\u003c/td\u003e\n    \u003ctd\u003eMedium\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en\u003c/td\u003e\n    \u003ctd\u003e56\u003c/td\u003e\n    \u003ctd\u003e$50K-100K\u003c/td\u003e\n    \u003ctd\u003eLow\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\nAnd so on.\n\nPretty simple, right? The code for this categorization is simple too (assuming you're using Pandas and Python; though it's also simple in SQL).\n\n```python\n# Here's one example\nimport numpy as np\nimport pandas as pd\n\ncdf['Income Bucket'] = pd.cut(cdf['Annual Income ($K)'], \n    bins=[0, 25, 35, 55, np.inf], \n    labels=['\u003c25', '25-35', '35-55', '55+']\n)\n```\nThis is a really helpful and simple way to understand our customers and it's the way that most businesses do analytics, but we can do more. üòä\n\n### 2. Algorithmic Segments\n\nSegments defined using simple business logic are great because they are so easy to interpret, but that's not free.\nBy favoring simplicity we have to limit ourselves to (potentially) suboptimal segments. \nThis is typically on purpose and entirely fine but, again, we can do better.\n\nSo how do we do better?\n\nCue statistics, data mining, analytics, machine learning, or whatever it's called this week. More specifically, we can use the classic [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering) algorithm to *learn* an optimal set of segments given some set of data.\n\nTo skip over many important details ([more here](https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3)), K-Means is an algorithm that optimally buckets your data into $K$ groups (according to a specific mathematical function called the [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)). It's a classic approach and tends to work quiet well in practice (there are a ton of other neat [clustering algorithms](https://en.wikipedia.org/wiki/Cluster_analysis#Algorithms)) but one non-technical challenge is (1) choosing $K$ and (2) explaining what a single cluster actually means to literally anyone else.\n\nSolving (1) is relatively straight-forward. You can run K-means for some number of $K$ from [0, $m$] ($m \u003e 0$ and choose what appears to be a $k$ that sufficiently minimizes the within-cluster sum-of-squares (i.e., $\\sum_{i=0}^{n} min_{\\mu_j \\in C}||x_i - \\mu_j||^2$). Here notice that the the majority of the variation of the clusters can be capture by $k=6$.\n\n![The Inertia Function!](inertia.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eInertia as a function of k\u003c/i\u003e\u003c/p\u003e\n\nNow to (2), which is the harder challenge. If I were to plot my data and look at the clusters, I'd have something that looks like:\n\n![K-Means!](kmeans.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eLook at all 3 of those beautiful dimensions!\u003c/i\u003e\u003c/p\u003e\n\nHow cool, right? This little algorithm learned pretty clear groups that you can see rather obviously in the data. Impressive! And also useless to your boss and colleagues.\n\nMore seriously, while you can see these clusters, you can't actually extract a clear description from it, which makes interpreting it really, really hard when you go past 3 dimensions.\n\nSo what can you do to make this slightly more meaningful?\n\nEnter [decision trees](https://en.wikipedia.org/wiki/Decision_tree). Another elegant, classic, and amazing algorithm. Decision Trees basically split up your data using simple `if-else` statements. So, a trick that you can use is to take the predicted clusters and run a Decision Tree (Classification) to predict the segment and use the learneed Tree's logic as your new business logic.\n\nI find this little trick pretty fun and effective since I can more easily describe how a machine learned a segment and I can also inspect it. Let's suppose I ran my tree on this learned K-means, what would the output look like?\n\n![Decision Tree Ouput!](decisiontree.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eIs this really more interpretable?\u003c/i\u003e\u003c/p\u003e\n\nThere you have it, now you have a segmentation that is closer to optimal and somewhat easier to interpret. It's still not as good as the business definition but you could actually read through this and eventually come up with a heuristic driven approach as well, which is why I like it and why I've used it in the past.\n\nAnd here's the code to run the K-means and the Decision tree.\n\n```python\nimport pydotplus\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \n\noptimal_clusters = 6\n# 6 clusters 6 colors\nxcolors = ['red', 'green', 'blue', 'orange', 'purple', 'gray']\n# Chose 6 as the best number of clusters\nkmeans_model = (KMeans(n_clusters = optimal_clusters,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nkmeans_model.fit(X1)\ncdf['pred_cluster_kmeans'] = kmeans_model.labels_\ncentroids = kmeans_model.cluster_centers_\n\ndisplay(pd.DataFrame(cdf['pred_cluster_kmeans'].value_counts(normalize=True)))\n\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X1, cdf['pred_cluster_kmeans'])\n\n# Predict the response for test dataset\ncdf['pred_class_dtree'] = clf.predict(X1)\n\ndisplay(pd.crosstab(cdf['pred_cluster_kmeans'], cdf['pred_class_dtree']))\ndot_data = StringIO()\nexport_graphviz(\n    decision_tree=clf, \n    out_file=dot_data,  \n    filled=True, \n    rounded=False,\n    impurity=False,\n    special_characters=True, \n    feature_names=xcol_labels, \n    class_names=cdf['pred_cluster_kmeans'].unique().astype(str).tolist(),\n\n)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(\"./decisiontree.png\")\n```\n## What can you do with your new segments?\n\nNow that we have our customer segments we can do all sorts of different things.\nWe can create [A/B tests](https://www.optimizely.com/optimization-glossary/ab-testing/) for website experiences or we can test the impact of [changing our prices](https://medium.com/@judaikawa/price-elasticity-statistical-modeling-in-the-retail-industry-a-quick-overview-fdab5350222) to certain customers.\nIn general, we can just try a bunch of new stuff.\n\n## How do I know if my segments are accurate?\n\nThe metric we used in the example above (i.e., within cluster sum-of-squares / inertia) was a reasonably straightforward way to measure the accuracy of your segments from an analytical perspective, but if you wanted to take a closer look, I'd recommend reviewing individual users in each segment. It sounds a little silly and can, in some cases, lead to the wrong conclusions but I firmly believe that in data science, you just have to really **look** at your data. You learn a lot from it.\n\n## How do I know when my segments need to change?\n\nLastly, segments can change; your customers are always evolving so it's good to re-evaluate your clusters time and again. The emergence of new segments should feel very obvious, since it may be driven by product or acquisition changes. As a concrete example, if you noticed that important businesss metrics split by your segments are starting to behave a little differently, then you can investigate whether it's driven by a change in the segments; sometimes it is, sometimes it's not.\n\n\n## Conclusion\nThis tutorial ended up being a little longer than I anticipated but oh well, I hope you enjoyed it.\n\nI've stored the code to reproduce this example in a [Jupyter Notebook](https://github.com/franciscojavierarceo/Python/blob/master/demos/Customer%20Segmentation%20Example.ipynb) available on my GitHub (note to render the interactive 3D visualization you have to run the notebook). To get it up and running you only need to download the notebook, [download the data](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python), install [Docker](https://www.docker.com/get-started), and simply run:\n\n```bash\ndocker run -it -p 8888:8888 -v ~/path/to/your/folder/:/home/jovyan/work --rm --name jupyter jupyter/scipy-notebook:17aba6048f44\n```\n\nAnd you should be good to go. Happy segmenting!\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*","excerpt":""},"previousPost":{"slug":"how-to-build-a-credit-risk-model","frontmatter":{"title":"How to Build a Credit Risk Model","description":"A Data Scientists Guide to Building a Basic Credit Risk Model","date":"January 31, 2021"},"excerpt":"","content":"\n*TL;DR: A Data Science Tutorial on building a Credit Risk model.*\n\nI previously [wrote](/post/data-science-and-fintech) about some of the work data scientists do in the fintech space, which briefly discussed [credit risk models](https://en.wikipedia.org/wiki/Credit_risk) but I wanted to write a more technical review and talk about what I think are the most important points.\n\nI spent most of my career at banks as a data scientist and built credit risk, fraud, and marketing models in the consumer and commercial space in the US, Australia, and South Africa. I learned a lot from those experiences, so I thought I'd share a simple example of how one of the core pieces of algorithmic/quantitative underwriting is done.\n\nIn this post, I'll try to answer the following questions:\n\n- What is a credit risk model?\n- What data does a credit risk model use?\n- How do you estimate a credit risk model?\n- How do you know if your model is performing well?\n- What are some common mistakes to avoid?\n- What are useful facts to know about credit risk models?\n\nIt's worth caveating up front that this is a very narrow take focused only on the analytical aspect of the work and there is an extraordinary amount of legal, compliance, and business work that I am intentionally omitting.\n\nWith that said, let's dive right in.\n\n## What is a Credit Risk Model?\n\nIn the consumer/retail space, a credit risk model tries to predict the probability that a consumer won't repay money that they've borrowed.\nA simple example of this is an [unsecured personal loan](https://www.investopedia.com/terms/u/unsecuredloan.asp). \n\nLet's suppose you submitted an application to borrow $5,000 from a lender. That lender would want to know the likelihood of [default](https://www.investopedia.com/terms/d/default2.asp) before deciding on (1) whether to give you the loan and (2) the price they want to charge you for borrowing the money. So that probability is probably quite important...but how do they come up with it?\n\n## What Data does a Credit Risk Model Use?\n\nLenders typically use data from the major [Credit Bureaus](https://www.investopedia.com/personal-finance/top-three-credit-bureaus/) that is [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act) compliant, which basically means that the legal and reputational risk of using this data is very low.\n\nTypically, you'd purchase a dataset from one of the bureaus (or use data inside one of their analytical sandboxes) and clean the dataset into something that looks like the following:\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eDefault\u003c/th\u003e\n    \u003cth\u003eInquiries in Last 6 Months\u003c/th\u003e\n    \u003cth\u003eCredit Utilization\u003c/th\u003e\n    \u003cth\u003eAverage Age of Credit\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eYes\u003c/td\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e0.8\u003c/td\u003e\n    \u003ctd\u003e12\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eNo\u003c/td\u003e\n    \u003ctd\u003e8\u003c/td\u003e\n    \u003ctd\u003e0.0\u003c/td\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nAnd so on.\n\nIn summary, it's just a bunch of data about your borrowing history.\nIt's a little recursive/cyclical because in order to grow your credit you need to have credit but let's ignore that detail.\n\nOne of the most important steps in the model development process is *precisely* defining **default** because this will eventually reflect the performance of your portfolio defining it thoughtfully, accurately, and confidently is extremely consequential.\n\nSo how do you define it?\n\nTime. \n\nIf you're planning to launch a term loan, you usually set boundaries on the duration of the loan. For revolving lines of credit (like a credit card), you simply apply a reasonable boundary on time that makes good business sense.\n\nLet's say you want to launch a 12 month loan to an underserved market, then you'd want to get historical data of other lenders to build your model on.\nIt sounds a little surprising that you can do this but that's basically how the bureaus make their money.\n\nAn important thing to keep in mind is that you need to make sure you pull the data at 2 different time periods: (1) when the original application was made so you can use data that is relevant for underwriting (and so you don't have forward-looking data resulting in [data leakage](https://www.kaggle.com/dansbecker/data-leakage)) and (2) 12 months later (or whatever time period is appropriate for you) to check if the consumer defaulted on their loan.\n\nThere's a lot more to it and you can expand on things in much more elegant ways to handle different phenomena but for the sake of simplicity, this is essentially how it's done.\n\nSo, after painfully cleaning up all that data, what do you do with it?\n\n## Building a Probability of Default Model\n\nNow that you have that glorious dataset you can start to run different [Logistic Regressions](https://en.wikipedia.org/wiki/Logistic_regression) or use other classification based [machine learning](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html) algorithms to find hidden patterns and relationships (i.e., [non-linear functions](https://blog.minitab.com/blog/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis#:~:text=If%20the%20equation%20doesn't,a%20linear%20equation%2C%20it's%20nonlinear.\u0026text=Thetas%20represent%20the%20parameters%20and,one%20parameter%20per%20predictor%20variable.) and [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics))).\n\nPersonally, this is the most intellectually engaging part of the work. The other work involved in credit risk modeling is usually very stressful and filled with less exciting graphs but here you get to pause, look at data, and, for a moment, try to make a crude approximation of the world. I find this part *fascinating*.\n\nIf we stick with our simple dataset above for our model, we could use our good old friend Python to run that Logistic Regression.\n\n```python\nimport numpy as np\nimport statsmodels.api as sm\n\n# Generating the data \nn = 10000\nnp.random.seed(0)\nx_1 = np.random.poisson(lam=5, size=n)\nx_2 = np.random.poisson(lam=2, size=n)\nx_3 = np.random.poisson(lam=12, size=n)\ne = np.random.normal(size=n, loc=0, scale=1.)\n\n# Setting the coefficient values to give us a ~5% default rate\nb_1, b_2, b_3 = -0.005, -0.03, -0.15\nylogpred =  x_1 * b_1 + x_2 * b_2 + x_3 * b_3 + e\nyprob = 1./ (1.+ np.exp(-ylogpred))\nyclass = np.where(yprob \u003e= 0.5, 1, 0)\nxs = np.hstack([\n    x_1.reshape(n, 1), \n    x_2.reshape(n, 1), \n    x_3.reshape(n, 1)\n])\n# Adding an intercept to the matrix\nxs = sm.add_constant(xs)\nmodel = sm.Logit(yclass, xs)\n# All that work just to run .fit(), how terribly uninteresting\nres = model.fit()\nprint(res.summary())\n\n         Current function value: 0.163863\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                10000\nModel:                          Logit   Df Residuals:                     9996\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 29 Jan 2021   Pseudo R-squ.:                  0.1056\nTime:                        00:08:17   Log-Likelihood:                -1638.6\nconverged:                       True   LL-Null:                       -1832.2\nCovariance Type:            nonrobust   LLR p-value:                 1.419e-83\n==============================================================================\n                 coef    std err          z      P\u003e|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4535      0.209      2.166      0.030       0.043       0.864\nx1            -0.0439      0.023     -1.949      0.051      -0.088       0.000\nx2            -0.0390      0.035     -1.109      0.267      -0.108       0.030\nx3            -0.3065      0.017    -18.045      0.000      -0.340      -0.273\n==============================================================================\n```\n\nWow, look at all of that beautiful, useless statistical output! \n\nIt's not *really* useless but 99% of the people involved will not find it useful. \nSo we probably need an alternative way to show and inform these results to non-technical stakeholders (but you as a data scientist can look at this as much as you'd like).\n\n![The Glorious Lift Chart!](liftchart.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eThe Beloved Lift Chart\u003c/i\u003e\u003c/p\u003e\n\nCue the [Lift Chart](https://www.casact.org/education/rpm/2016/presentations/PM-LM-4.pdf). This chart may look fancy but it's actually pretty simple, it's just [deciling](https://www.investopedia.com/terms/d/decile.asp) your data (i.e., sorting and bucketing into 10 equal groups) according to the *predicted* default rate from your model. It's worth noting that this is a quantized and reversed version of the [ROC chart](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and they represent the same information.\n\nThere are 3 important pieces of information from this graph: \n\n1. The AUC tells you the performance\n2. The closer the two lines are together the more accurate the probability estimate\n3. The steeper the slope at the higher deciles, the better the rank order separation\n\nIf you had perfect information, you'd get a graph that looks like this:\n\n![The Perfect Lift Chart!](liftchart_optimal.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eIf your model looks like this, you've done something terribly wrong\u003c/i\u003e\u003c/p\u003e\n\nAnd here's the code to generate those graphs.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\ndef liftchart(df: pd.DataFrame, actual: str, predicted: str, buckets: int=10) -\u003e None:\n    # Bucketing the predictions (Deciling is the default)\n    df['predbucket'] = pd.qcut(x=df[predicted], q=buckets)\n    sdf = df[[actual, predicted, 'predbucket']].groupby(\n        by=['predbucket']).agg({\n        actual: [np.mean, sum, len], \n        predicted: np.mean\n        }\n    )\n    aucperf = roc_auc_score(df[actual], df[predicted])\n    sdf.columns = sdf.columns.map(''.join) # I hate pandas multi-indexing\n    sdf = sdf.rename({\n        actual + 'mean': 'Actual Default Rate', \n        predicted + 'mean': 'Predicted Default Rate'\n    }, axis=1)\n    sdf[['Actual Default Rate', 'Predicted Default Rate']].plot(\n        kind='line', style='.-', grid=True, figsize=(12, 8), \n        color=['red', 'blue']\n    )\n    plt.ylabel('Default Rate')\n    plt.xlabel('Decile Value of Predicted Default')\n    plt.title('Actual vs Predicted Default Rate sorted by Predicted Decile \\nAUC = %.3f' % aucperf)\n    plt.xticks(\n        np.arange(sdf.shape[0]), \n        sdf['Predicted Default Rate'].round(3)\n    )\n    plt.show()\n\n# Using the data and model from before!\npdf = pd.DataFrame(xs, columns=['intercept', 'x1', 'x2', 'x3'])\npdf['preds'] = res.predict(xs)\npdf['actual'] = yclass\n\n# Finally, what we all came here to see\nliftchart(pdf, 'actual', 'preds')\n\n# This is what it looks like when we have perfect information\npdf['truth'] = pdf['actual'] + np.random.uniform(low=0, high=0.001, size=pdf.shape[0])\nliftchart(pdf, 'actual', 'truth', 10)\n```\n\u003ccenter\u003e\u003ci\u003eJudge me not by the elegance of my code but by the fact that it runs.\u003c/i\u003e\u003c/center\u003e\n\n## Evaluating your Default Model\n\nSo this visualization is helpful, but how do you quantify the performance into a single number? Well, how about [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)?\n\nIt turns out that Accuracy isn't really a great metric when you have a low default rate (or more generally when you have severe [class imbalance](https://en.wikipedia.org/wiki/Accuracy_paradox)). As an example suppose you have a 5% default rate, that means 95% of your data did *not default*, so if your model predicted that no one defaulted, you'd have 95% accuracy.\n\nAccurate, obvious, and entirely useless. Without proper adjustment, this behavior is actually very likely to occur in your model, so we tend to ignore the accuracy metric and instead we focus on the rank order seperation/predictive power of the model.\n\nTo measure that predictive power, there are 4 metrics industry professionals typically look at to summarize their model: [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), the [Kolomogorov-Smirnov (KS) Test](https://en.wikipedia.org/wiki/Kolmogorov‚ÄìSmirnov_test), and [Gini/AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n\nOne point of humor that has been a surprisingly common topic of discussion in my career is the equivalence of Gini and AUC. Bankers like Gini, for whatever inertia related reason, but it's equivalent to AUC via:\n\n$$Gini = 2 * AUC - 1$$ \n\nand obviously\n\n$$AUC = \\frac{Gini+1}{2}$$.\n\nGini is bound between [-1, 1] and AUC between [0, 1] but technically if your AUC is less than 0.5 (and \u003c 0 for Gini) that means you're doing worse than random ([and you could do better by literally doing the opposite of what your model says](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)) so most people will say AUC is between [0.5, 1] and that Gini is between [0, 1].\n\nA good model is usually around 70% AUC / 40% Gini. The higher the better.\n\nIt's always useful to look at your lift chart as it can tell you a lot about how predictive the model is at different deciles. In some cases, your model may not have enough variation in the attributes/features/predictors to differentiate your data meaningfully. The metric won't show you that, only a glance at the lift chart will, so it's always a good to review it.\n\nOne important point here is that any single metric is very crude and a data set can be pathologically constructed to break it, so while these metrics and charts are very helpful, there are cases where things can still misbehave even though they seem normal.\n\nNow that you have your exciting new default model, you can use it in your underwriting strategy to differentiate amongst your competitors on pricing, approve/decline, and targeting customers, right?\n\nNot quite.\n\n## Common Mistakes and Pitfalls\n\nBefore you get too excited about your model, you want to verify that it's behaving logically, so I thought I'd list some items that you will want to check against to make sure nothing disastrous will happen. \n\n- Double check for [Data Leakage](https://www.kaggle.com/dansbecker/data-leakage)\n- Verify that you don't have any [data errors](https://www.datasciencecentral.com/profiles/blogs/common-errors-in-machine-learning-due-to-poor-statistics-knowledg)\n- Make sure you've done your best not to fall for [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\n- Validate your model with an [Out of Time](https://statmodeling.stat.columbia.edu/2016/11/22/30560/) Hold Out Sample\n- Confirm your model actually has a [representative sample](https://www.investopedia.com/terms/r/representative-sample.asp#:~:text=A%20representative%20sample%20is%20a,three%20males%20and%20three%20females.) of your future portfolio\n- Make sure you're not time traveling \n    - This is a form of Data Leakage (Leaky Predictors) and it's basically making sure you don't use data that's in the future of what you're representing (except the default flags that you're trying to predict/model)\n- Understand the correlation your *predicted default* may have to your other products in your portfolio\n    - This is not important from a statistical perspective but it is very important for your business\n- Always be skeptical if your model performs too well\n    - This is very likely data leakage and can be very embarrassing if you over-excite management\n\nI could have written a book ([recommendation here](https://www.amazon.com/Credit-Risk-Modelling-Theoretical-Foundations-Diagnostic-ebook/dp/B07FZGG63V)) with a much longer set of information both from a statistical and business lense but, for the sake of brevity, I wrote the most pressing ones and I invite you to search for more information about other important details.\n\n## Some other Interesting Extensions\n\n\n### Model Extensions\nThe model I showed above is quite literally the most basic example and the models that most lenders and vendors use are much, much more sophisticated. Many of them have enhanced their models to handle [panel data](https://en.wikipedia.org/wiki/Panel_data) (i.e., cohorts of people over time) and can construct models that are extremely robust.\n\nThe best class of models in this elite category (in my personal opinion) are [Discrete Time Survival Models](https://bookdown.org/content/4253/fitting-basic-discrete-time-hazard-models.html). Actually, this isn't *my* opinon, this wisdom was shared with me from two of the former heads of statistical modeling at Capital One, so don't trust my judgement, trust theirs.\n\n### Quantization and the Weight of Evidence\nAnother interesting implementation detail is that statisticians/data scientists in the credit scoring space often transform the inputs/attributes/features of their model via [quantization](https://en.wikipedia.org/wiki/Quantization). More specifically through a transformation called the [Weight of Evidence](https://multithreaded.stitchfix.com/blog/2015/08/13/weight-of-evidence/). It's neat, troglodytic, and surprisingly effective.\n\n\n### Adverse Action Reasons\nCredit risk modeling requires the underwriter to grant adverse action codes in the case of rejecting a customer who applies for a credit product (this is a driven by the FCRA). Doing this algorithmitically is very doable and different organizations do it slightly differently but the short version is you take the absolute largest partial predictions (i.e., $argmax_{x}f(x_j) = |x_j * \\beta_j|$,  $\\forall j=1,..., k$).\n\n### Model Complexity and Usefulness\nLastly, when building these statistical models the most important thing to think about is the trade-off between complexity and performance on your target population. Typically in the lending space you will not approve all of your customers, so obsessing over model fit on the proportion of your population that won't be approved is not always useful. It's a small point, but extremely consequential and will hopefully save you some time.\n\n## Conclusion\n\nThis tutorial was not nearly as code heavy as I would have liked (only showing a very silly regression) but I felt like the content was still rather dense ([the full code is available here](https://github.com/franciscojavierarceo/Python/blob/master/demos/credit-model-demo.py)).\n\nPeople spend their careers studying these statistical models and the important consequences of them, so I really want to emphasize how much I have trivialized the work here only for the sake of simplicity. I have not talked about how you can improve the model when it's underperforming or how one can try to measure biases that may occur, but these are important pieces for society.\n\nI will conclude by saying that these models are heavily governed by many layers of regulation. As a statistician and a minority, I think this is a good thing.\nThat's not to say that harm isn't caused by these models because I think there is but I do think machines are more easily governed, evaluated, and modified...unlike human decision making which can be subjective and prone to unknown biases that are difficult to quantify ([here's some additional reading material](https://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf)).\n\nI hope you found this post useful and interesting.\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*"},"nextPost":{"slug":"docker-for-data-science","frontmatter":{"title":"How to use Docker to Launch a Jupyter Notebook","description":"A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook","date":"February 13, 2021"},"excerpt":"","content":"\n*TL;DR: A Data Science Tutorial on the benefits of Docker.*\n\n## Some History\nI began my foray into what's now called [Data Science](https://en.wikipedia.org/wiki/Data_science) back in 2011. I was doing my first master's in economics and statistics and I was doing econometrics research on consumer demand based on survey data using [SAS](https://www.sas.com/en_us/company-information.htmlhttps://www.sas.com/en_us/company-information/profile.html).\n\n![Look at that rise!](data-science-google-trends.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eLooks like I graduated at an interesting time.\u003c/i\u003e\u003c/p\u003e\n\nTechnology was *much* different then, distributed computing and Open Source Software (OSS) was only starting to get the popularity and attention it has now. More practically, most businesses weren't using cloud services, they were using their own servers for storing and managing their data (i.e., real physical machines) with fixed RAM and a lot of overhead (read: chaos when the power goes out).\n\nThe most sophisticated analytical shops used SAS* to process their data since it was a very efficient way to analyze large data out of memory.\n\nBut it wasn't fault tolerant or stable. Software libraries for different mathematical frameworks have evolved so much over time and they just kept changing, so the infrastructure kept changing, too.\n\nIn short, the way data scientists did analytics was pretty brittle: most places didn't use version control or servers; code was sent via emails; and deploying models was usually done in an Oracle/MySQL table that ran a query, joins, and a sum-product. It was the Wild West.\n\nCloud computing and OSS changed the game. [R](https://cran.r-project.org), [Python](https://www.python.org), [Hadoop](https://hadoop.apache.org), [Spark](https://spark.apache.org), [CUDA](https://developer.nvidia.com/cuda-toolkit), and other frameworks completely influenced how we thought about that infrastructure.\n\nPython, in particular, has been one of the greatest contributions to data science and it has truly helped push the field further.\n\n## Python, the Beautiful\n\nPython wasn't the original data science language, R and SAS were much more popular back in the early and mid-2010s but two popular data mining libraries helped Python skyrocket in the data science community ([sklearn](https://scikit-learn.org/stable/) and [xgboost](https://en.wikipedia.org/wiki/XGBoost)). Then in 2015 people made advances in deep learning frameworks (moving away from [Theano](https://en.wikipedia.org/wiki/Theano_(software))) and creating things like [Caffe](https://caffe.berkeleyvision.org), [Keras](https://en.wikipedia.org/wiki/Keras), [Tensorflow](https://en.wikipedia.org/wiki/TensorFlow), and eventually [PyTorch](https://en.wikipedia.org/wiki/PyTorch) (my personal favorite).\n\nAll of the stuff under the hood changed dramatically and it made the infrastructure around deploying these models change dramatically, too.\n\nThe ever-evolving software made getting data science infrastructure up and running really annoying, time consuming, and eventually kind of wasteful because it would get stale quickly, but the world has evolved again.\n\nCue [Docker](https://en.wikipedia.org/wiki/Docker_(software)) and the emergence of [containerization](https://hackernoon.com/what-is-containerization-83ae53a709a6).\n\n## Docker and Jupyter Notebooks\n\nDocker is basically a way to easily configure a mini-computer in your computer. The idea being, that if you configure it with a single file declaring what stuff (i.e., software) you need in it, you can deploy that same container to some production environment. In real, customer-facing applications even a small subversion change of a single library can break your entire service.\n\nDocker can help mitigate that risk.\n\n![It works on my machine!](docker.jpg)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eThis meme is suprisingly accurate.\u003c/i\u003e\u003c/p\u003e\n\nWhat's interesting is that people have made attempts at doing similar things for a long time ([virtual environments](https://virtualenv.pypa.io/en/latest/), [Pyenv](https://github.com/pyenv/pyenv), [virtual box](https://www.virtualbox.org), etc.) and while most of them were helpful, they all still had really annoying issues come up constantly...but Docker is much more comprehensive.\n\nSo how is that relevant for Python and data scientists? Well, if you're doing data science work, odds are you're probably using Python or R and you might be doing that work using a [Jupyter Notebook](https://jupyter.org). \n\nThe [Jupyter Project](https://jupyter.org/about) has created a wonderful [Data Science](https://hub.docker.com/r/jupyter/datascience-notebook/) docker image that allows you to trivially get up and running.\n\nAll you have to do is [install Docker](https://docs.docker.com/engine/install/) and run the following in your terminal:\n\n```bash\ndocker run -it -p 8888:8888 -v /your/folder/:/home/jovyan/work --rm --name jupyter jupyter/datascience-notebook\n```\nYou'll see some output and at the end of it you should see something like: \n\n```\n[C 2021-02-14 14:37:06.596 ServerApp] \n    \n    To access the server, open this file in a browser:\n        file:///home/jovyan/.local/share/jupyter/runtime/jpserver-6-open.html\n    Or copy and paste one of these URLs:\n        http://7c94e4cf2dc1:8888/lab?token=this-will-be-a-magical-token\n     or http://127.0.0.1:8888/lab?token=this-will-be-a-magical-token\n```\nClick on that link in your terminal and you should be directed to a page that looks like the image below:\n\n![So many options!](docker-for-data-science.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eWho is using Julia these days?\u003c/i\u003e\u003c/p\u003e\n\nAnd there you have it, you can start ripping through all sorts of data in minutes!\n\nA great benefit of this particular docker image is that it has most of the Python/R libraries you want already out of the box but if you want to add another, you can do that right in your notebook by using [Jupyter Magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) in a cell like so:\n```bash\n%pip install snowflake\n```\n\n![Simple install](pip_install.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eWow, that was easy.\u003c/i\u003e\u003c/p\u003e\n\nAnd now you can use that library. See how nice it is not to have to [dual boot](https://en.wikipedia.org/wiki/Multi-booting) your computer to install something?\n\n## Conclusion\n\nData science infrastructure is going to continue to evolve very heavily, so I imagine this post will be outdated in 2 years but currently this is an extremely fast and painless way to get up and running.\n\nI can't emphasize enough how miserable managing different Operating System (OS), Python, or other software library versions really is. Debugging these things used to take days or weeks and now it's just trivial, so I'd really recommend this approach. An added benefit is that this will also save you an extraordinary amount of time when you go to deploy your model...but more on that later. üòâ\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*\n\n---\n*As a brief side note, the history of SAS is amazing and I really recommend reading the [Wikipedia page](https://en.wikipedia.org/wiki/SAS_Institute) on it. Most of the large banks, pharmaceutical firms, several government agencies, research institutions, and other major organizations in the world still operate on SAS because it's so deeply embedded into their business. Now, that technology can no longer be decoupled from their core infrastructure, which is interesting.*"}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"customer-segmentation-data-science"},"buildId":"kJxfpRQQyGIN4m6m9HTPj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>