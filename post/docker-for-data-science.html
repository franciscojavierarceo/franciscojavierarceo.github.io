<!DOCTYPE html><html lang="en-US"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if(!e)return localStorage.setItem('theme','system'),d.add('system');if("system"===e){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else d.add(e)}catch(e){}}()</script><title>How to use Docker to Launch a Jupyter Notebook</title><meta name="description" content="A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook"/><meta property="og:type" content="website"/><meta name="og:title" property="og:title" content="How to use Docker to Launch a Jupyter Notebook"/><meta name="og:description" property="og:description" content="A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="How to use Docker to Launch a Jupyter Notebook"/><meta name="twitter:description" content="A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook"/><meta name="twitter:creator" content="franciscojarceo"/><link rel="icon" type="image/png" href="/favicon.ico"/><link rel="apple-touch-icon" href="/favicon.ico"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/a1ec42729d4b02f29c9a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a1ec42729d4b02f29c9a.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-39418da170c6bb422e22.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.a6f6c255f30e2cc8ab2c.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-2e7721256aae47a12b75.js" as="script"/><link rel="preload" href="/_next/static/chunks/5e7de4f36e438e169c5d145e7df90137ae956f1e.c55db21543619f2e3353.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/post/%5Bslug%5D-691413d01639bc43c6a6.js" as="script"/></head><body><div id="__next"><div class="w-full min-h-screen dark:bg-gray-700 dark:text-white"><div class="max-w-screen-md px-4 py-12 mx-auto antialiased font-body"><header class="flex items-center justify-between  mb-2"><meta name="description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="keywords" content="francisco javier arceo, data science, finance, fintech, engineering, django, python"/><meta property="og:type" content="website"/><meta property="og:title" content="Francisco&#x27;s Random Thoughts"/><meta property="og:locale" content="en_US"/><meta property="og:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta property="og:image" content="https://og-image.now.sh/Francisco&#x27;s%20Random%20Thoughts.png?theme=light&amp;md=0&amp;fontSize=75px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="twitter:description" content="My chaotic thoughts on computers, statistics, finance, and data"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="franciscojarceo"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-71125809-1/"></script><script src="/ga.js"></script><div class="max-w-md"><a><a class="text-2xl font-black text-black no-underline font-display dark:text-white" href="/">Francisco Javier Arceo</a></a></div><script></script></header><main style="padding-bottom:10px"><article><header class="mb-8"><h1 class="mb-2 text-6xl font-black leading-none font-display">How to use Docker to Launch a Jupyter Notebook</h1><p class="text-sm">February 13, 2021</p></header><div class="mb-4 prose lg:prose-lg dark:prose-dark"><p><em>TL;DR: A Data Science Tutorial on building the benefits of Docker.</em></p><h2>Some History</h2><p>I began my foray into what&#x27;s now called <a href="https://en.wikipedia.org/wiki/Data_science">Data Science</a> back in 2011. I was doing my first master&#x27;s in economics and statistics and I was doing econometrics research on consumer demand based on survey data, using <a href="https://en.wikipedia.org/wiki/SAS_Institute">SAS</a>.</p><p>Technology was <em>much</em> different then, distributed computing and Open Source Software (OSS) was only starting to get the popularity and attention it has now. And more practically, most businesses weren&#x27;t using cloud services. Most businesses were still using their own servers for storing and managing their data (i.e., real physical machines) with fixed RAM and a lot of overhead (read: chaos when the power goes out).</p><p>For the most sophisticated analytical shops, they used SAS to process their data, since it was a very efficient way to analyze data out of memory.</p><p><em>As a brief side note, the history of SAS is amazing and I really recommend reading the Wikipedia page on it. Most of the large banks in the world still operate on SAS because it&#x27;s been around so long. Now, that technology can no longer be decoupled from core banking infrastructure, which is interesting.</em></p><p>But it wasn&#x27;t fault tolerant or stable. Software libraries for different mathematical frameworks have evolved so much over time and they just kept changing, so the infrastructure kept changing, too. In short, the way data scientists did analytics was pretty brittle. Most places didn&#x27;t use version control or servers. Code was sent via emails and deploying models was usually done in an Oracle/MySQL table that ran a query, joins, and a sum-product (if at all). It was the wild west.</p><p>Cloud computing and OSS changed the game. R, Python, Hadoop, Spark, CUDA, and other frameworks completely trivialized so much of that infrastructure.</p><p>Python, in particular, has been one of the greatest contributions to data science and it has truly helped push the field further.</p><h2>Python, the Beautiful</h2><h2>Docker and Jupyter Notebooks</h2><pre style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1</span><span>docker run -it -p 8888:8888 -v /your/folder/:/home/jovyan/work --rm --name jupyter jupyter/datascience-notebook</span></code></pre></div></article></main><footer class="text-lg font-light"><div style="padding-top:10px;padding-bottom:10px"><a class="text-lg font-bold" href="/">← Back home</a></div><hr/><div><p>Like this blog? Check out the code on my<!-- --> <a href="https://github.com/franciscojavierarceo/franciscojavierarceo.github.io">GitHub</a>.</p><p>Built with<!-- --> <a href="https://nextjs.org/">Next.js</a> and ☕</p></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"How to use Docker to Launch a Jupyter Notebook","description":"A Data Scientists Guide to using Docker containers to quickly spin up a Jupyter Notebook","date":"February 13, 2021"},"post":{"content":"\n*TL;DR: A Data Science Tutorial on building the benefits of Docker.*\n\n## Some History\nI began my foray into what's now called [Data Science](https://en.wikipedia.org/wiki/Data_science) back in 2011. I was doing my first master's in economics and statistics and I was doing econometrics research on consumer demand based on survey data, using [SAS](https://en.wikipedia.org/wiki/SAS_Institute).\n\nTechnology was *much* different then, distributed computing and Open Source Software (OSS) was only starting to get the popularity and attention it has now. And more practically, most businesses weren't using cloud services. Most businesses were still using their own servers for storing and managing their data (i.e., real physical machines) with fixed RAM and a lot of overhead (read: chaos when the power goes out).\n\nFor the most sophisticated analytical shops, they used SAS to process their data, since it was a very efficient way to analyze data out of memory. \n\n*As a brief side note, the history of SAS is amazing and I really recommend reading the Wikipedia page on it. Most of the large banks in the world still operate on SAS because it's been around so long. Now, that technology can no longer be decoupled from core banking infrastructure, which is interesting.*\n\nBut it wasn't fault tolerant or stable. Software libraries for different mathematical frameworks have evolved so much over time and they just kept changing, so the infrastructure kept changing, too. In short, the way data scientists did analytics was pretty brittle. Most places didn't use version control or servers. Code was sent via emails and deploying models was usually done in an Oracle/MySQL table that ran a query, joins, and a sum-product (if at all). It was the wild west.\n\nCloud computing and OSS changed the game. R, Python, Hadoop, Spark, CUDA, and other frameworks completely trivialized so much of that infrastructure.\n\nPython, in particular, has been one of the greatest contributions to data science and it has truly helped push the field further.\n\n## Python, the Beautiful\n\n\n## Docker and Jupyter Notebooks\n\n```\ndocker run -it -p 8888:8888 -v /your/folder/:/home/jovyan/work --rm --name jupyter jupyter/datascience-notebook\n```\n","excerpt":""},"previousPost":{"slug":"customer-segmentation-data-science","frontmatter":{"title":"Customer Segmentation using Data Science","description":"A Data Scientists Guide to Segmenting your Customers using clustering algorithms and decision trees.","date":"February 6, 2021"},"excerpt":"","content":"\n*TL;DR: A Data Science Tutorial on using K-Means and Decision Trees together.*\n\nCustomer segmentation (sometimes called [Market Segmentation](https://en.wikipedia.org/wiki/Market_segmentation)) is ubiqutous in the private sector. We think about bucketing people into $k$ mutually exclusive and collectively exhausting (MECE) groups. The premise being that instead of having 1 strategy for delivering a product or experience, providing $k$ experiences or strategies will yield much better engagement or acquisition from our customers.\n\nGenerally speaking, this makes sense; it's intuitive. Provide people a more curated experience and they will enjoy it more...and the more personalized the better. \n\nNetflix, Spotify, YouTube, Twiter, Instagram, and all of the big tech companies have mastered personalization by using robust, computationally expensive, and sophisticated machine learning pipelines. But the world has been doing this for a long time, just a much less sophisticated version.\n\nSo I thought I'd give a technical demo of what customer segmentation looks like in a basic way using a trick I've used for years.\n\nHere are the things I'd like to cover during this demo:\n\n1. What options do I have to segment my customers?\n2. How do I actually do the segmentation?\n3. What can I do with my new customer segments?\n4. How do I know that my segments are effective?\n5. How do I know when my segments have changed?\n\n## Approaches to Customer Segmentation\n\nThe phrase \"Customer Segments\" tends to mean different things across different industries, organizations, and even across business functions (e.g., marketing, risk, product, etc.). \n\nAs an example, for a consumer products retailer, they may refer to customer segments using both demographic information or their purchase behavior, where a lender may refer to their segments based on credit score bands. While very meaningfully different from a business perspective, the same algorithms can be used for both problems.\n\nAnalytically speaking, I've seen Customer Segments defined really in two main ways: (1) Business Segments and (2) Algorithmic Segments. Usually executives refer to their segments in the first category and data scientists focus on the second. The first is really important organizationally because 99% of the people working with your customers don't care about how you bucketed them and customers are the most important thing. Always.\n\n...but how do you *actually* (i.e., in code and data) get to those segments?\n\n### 1. Logical Business Segments\nThese segments tend to be defined by heuristics and things that make common sense. They are often built on things that are aligned with the goal of the business.\n\nHere are some examples:\n\n- The age of the customer (in years)\n- The income of the customer (in dollars or thousands of dollars)\n- The amount of money a customer spent in the last year\n- The likelihood a customer will spend money at a given store (purchase propensity / propensity to buy)\n- The customer's geographic region (e.g., zipcode, state)\n\nIn data, some of that customer information would look something like this:\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eUser ID\u003c/th\u003e\n    \u003cth\u003eAge\u003c/th\u003e\n    \u003cth\u003eCustomer Income\u003c/th\u003e\n    \u003cth\u003ePurchase Propensity\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e1\u003c/td\u003e\n    \u003ctd\u003e25\u003c/td\u003e\n    \u003ctd\u003e$45,000\u003c/td\u003e\n    \u003ctd\u003e0.9\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e30\u003c/td\u003e\n    \u003ctd\u003e$80,000\u003c/td\u003e\n    \u003ctd\u003e0.4\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en\u003c/td\u003e\n    \u003ctd\u003e56\u003c/td\u003e\n    \u003ctd\u003e$57,000\u003c/td\u003e\n    \u003ctd\u003e0.1\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\nAnd so on.\n\nWe could apply some logic/rules/code to create segment like:\n\n- Age Buckets\n    1. \u003c 25\n    2. 25-35\n    3. 35-55\n    4. 55+\n- Income Buckets\n    1. \u003c $25K\n    2. $25K-50K\n    3. $50K-100K\n    4. $100-150K\n    5. $150K+\n- Propensity Buckets\n    1. Low: [0, 0.25]\n    2. Medium: [0.25, 0.75]\n    3. High: [0.75, 1.0]\n\nAnd map that logic into our data, which would yield\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eUser ID\u003c/th\u003e\n    \u003cth\u003eAge Bucket\u003c/th\u003e\n    \u003cth\u003eIncome Bucket\u003c/th\u003e\n    \u003cth\u003ePropensity Bucket\u003c/th\u003e\n    \u003cth\u003e...\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e1\u003c/td\u003e\n    \u003ctd\u003e25-35\u003c/td\u003e\n    \u003ctd\u003e$25K-50K\u003c/td\u003e\n    \u003ctd\u003eHigh\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e2\u003c/td\u003e\n    \u003ctd\u003e25-35\u003c/td\u003e\n    \u003ctd\u003e$50K-100K\u003c/td\u003e\n    \u003ctd\u003eMedium\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en\u003c/td\u003e\n    \u003ctd\u003e56\u003c/td\u003e\n    \u003ctd\u003e$50K-100K\u003c/td\u003e\n    \u003ctd\u003eLow\u003c/td\u003e\n    \u003ctd\u003e...\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\nAnd so on.\n\nPretty simple, right? The code for this categorization is simple too (assuming you're using Pandas and Python; though it's also simple in SQL).\n\n```python\n# Here's one example\nimport numpy as np\nimport pandas as pd\n\ncdf['Income Bucket'] = pd.cut(cdf['Annual Income ($K)'], \n    bins=[0, 25, 35, 55, np.inf], \n    labels=['\u003c25', '25-35', '35-55', '55+']\n)\n```\nThis is a really helpful and simple way to understand our customers and it's the way that most businesses do analytics, but we can do more. 😊\n\n### 2. Algorithmic Segments\n\nSegments defined using simple business logic are great because they are so easy to interpret, but that's not free.\nBy favoring simplicity we have to limit ourselves to (potentially) suboptimal segments. \nThis is typically on purpose and entirely fine but, again, we can do better.\n\nSo how do we do better?\n\nCue statistics, data mining, analytics, machine learning, or whatever it's called this week. More specifically, we can use the classic [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering) algorithm to *learn* an optimal set of segments given some set of data.\n\nTo skip over many important details ([more here](https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3)), K-Means is an algorithm that optimally buckets your data into $K$ groups (according to a specific mathematical function called the [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)). It's a classic approach and tends to work quiet well in practice (there are a ton of other neat [clustering algorithms](https://en.wikipedia.org/wiki/Cluster_analysis#Algorithms)) but one non-technical challenge is (1) choosing $K$ and (2) explaining what a single cluster actually means to literally anyone else.\n\nSolving (1) is relatively straight-forward. You can run K-means for some number of $K$ from [0, $m$] ($m \u003e 0$ and choose what appears to be a $k$ that sufficiently minimizes the within-cluster sum-of-squares (i.e., $\\sum_{i=0}^{n} min_{\\mu_j \\in C}||x_i - \\mu_j||^2$). Here notice that the the majority of the variation of the clusters can be capture by $k=6$.\n\n![The Inertia Function!](inertia.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eInertia as a function of k\u003c/i\u003e\u003c/p\u003e\n\nNow to (2), which is the harder challenge. If I were to plot my data and look at the clusters, I'd have something that looks like:\n\n![K-Means!](kmeans.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eLook at all 3 of those beautiful dimensions!\u003c/i\u003e\u003c/p\u003e\n\nHow cool, right? This little algorithm learned pretty clear groups that you can see rather obviously in the data. Impressive! And also useless to your boss and colleagues.\n\nMore seriously, while you can see these clusters, you can't actually extract a clear description from it, which makes interpreting it really, really hard when you go past 3 dimensions.\n\nSo what can you do to make this slightly more meaningful?\n\nEnter [decision trees](https://en.wikipedia.org/wiki/Decision_tree). Another elegant, classic, and amazing algorithm. Decision Trees basically split up your data using simple `if-else` statements. So, a trick that you can use is to take the predicted clusters and run a Decision Tree (Classification) to predict the segment and use the learneed Tree's logic as your new business logic.\n\nI find this little trick pretty fun and effective since I can more easily describe how a machine learned a segment and I can also inspect it. Let's suppose I ran my tree on this learned K-means, what would the output look like?\n\n![Decision Tree Ouput!](decisiontree.png)\n\u003cp align=\"center\" style=\"padding:0\"\u003e\u003ci\u003eIs this really more interpretable?\u003c/i\u003e\u003c/p\u003e\n\nThere you have it, now you have a segmentation that is closer to optimal and somewhat easier to interpret. It's still not as good as the business definition but you could actually read through this and eventually come up with a heuristic driven approach as well, which is why I like it and why I've used it in the past.\n\nAnd here's the code to run the K-means and the Decision tree.\n\n```python\nimport pydotplus\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \n\noptimal_clusters = 6\n# 6 clusters 6 colors\nxcolors = ['red', 'green', 'blue', 'orange', 'purple', 'gray']\n# Chose 6 as the best number of clusters\nkmeans_model = (KMeans(n_clusters = optimal_clusters,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nkmeans_model.fit(X1)\ncdf['pred_cluster_kmeans'] = kmeans_model.labels_\ncentroids = kmeans_model.cluster_centers_\n\ndisplay(pd.DataFrame(cdf['pred_cluster_kmeans'].value_counts(normalize=True)))\n\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X1, cdf['pred_cluster_kmeans'])\n\n# Predict the response for test dataset\ncdf['pred_class_dtree'] = clf.predict(X1)\n\ndisplay(pd.crosstab(cdf['pred_cluster_kmeans'], cdf['pred_class_dtree']))\ndot_data = StringIO()\nexport_graphviz(\n    decision_tree=clf, \n    out_file=dot_data,  \n    filled=True, \n    rounded=False,\n    impurity=False,\n    special_characters=True, \n    feature_names=xcol_labels, \n    class_names=cdf['pred_cluster_kmeans'].unique().astype(str).tolist(),\n\n)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(\"./decisiontree.png\")\n```\n## What can you do with your new segments?\n\nNow that we have our customer segments we can do all sorts of different things.\nWe can create [A/B tests](https://www.optimizely.com/optimization-glossary/ab-testing/) for website experiences or we can test the impact of [changing our prices](https://medium.com/@judaikawa/price-elasticity-statistical-modeling-in-the-retail-industry-a-quick-overview-fdab5350222) to certain customers.\nIn general, we can just try a bunch of new stuff.\n\n## How do I know if my segments are accurate?\n\nThe metric we used in the example above (i.e., within cluster sum-of-squares / inertia) was a reasonably straightforward way to measure the accuracy of your segments from an analytical perspective, but if you wanted to take a closer look, I'd recommend reviewing individual users in each segment. It sounds a little silly and can, in some cases, lead to the wrong conclusions but I firmly believe that in data science, you just have to really **look** at your data. You learn a lot from it.\n\n## How do I know when my segments need to change?\n\nLastly, segments can change; your customers are always evolving so it's good to re-evaluate your clusters time and again. The emergence of new segments should feel very obvious, since it may be driven by product or acquisition changes. As a concrete example, if you noticed that important businesss metrics split by your segments are starting to behave a little differently, then you can investigate whether it's driven by a change in the segments; sometimes it is, sometimes it's not.\n\n\n## Conclusion\nThis tutorial ended up being a little longer than I anticipated but oh well, I hope you enjoyed it.\n\nI've stored the code to reproduce this example in a [Jupyter Notebook](https://github.com/franciscojavierarceo/Python/blob/master/demos/Customer%20Segmentation%20Example.ipynb) available on my GitHub (note to render the interactive 3D visualization you have to run the notebook). To get it up and running you only need to download the notebook, [download the data](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python), install [Docker](https://www.docker.com/get-started), and simply run:\n\n```bash\ndocker run -it -p 8888:8888 -v ~/path/to/your/folder/:/home/jovyan/work --rm --name jupyter jupyter/scipy-notebook:17aba6048f44\n```\n\nAnd you should be good to go. Happy segmenting!\n\n*Have some feedback? Feel free to [let me know](https://twitter.com/franciscojarceo)!*"},"nextPost":null},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"docker-for-data-science"},"buildId":"3ZPGVNAeWG2W3teh2yjvk","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-fa276ba060a4a8ac7eef.js"></script><script src="/_next/static/chunks/main-39418da170c6bb422e22.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.a6f6c255f30e2cc8ab2c.js" async=""></script><script src="/_next/static/chunks/pages/_app-2e7721256aae47a12b75.js" async=""></script><script src="/_next/static/chunks/5e7de4f36e438e169c5d145e7df90137ae956f1e.c55db21543619f2e3353.js" async=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-691413d01639bc43c6a6.js" async=""></script><script src="/_next/static/3ZPGVNAeWG2W3teh2yjvk/_buildManifest.js" async=""></script><script src="/_next/static/3ZPGVNAeWG2W3teh2yjvk/_ssgManifest.js" async=""></script></body></html>